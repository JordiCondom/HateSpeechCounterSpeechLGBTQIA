{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "203af05d-b1ee-48a7-84c4-ef4e5b214458",
   "metadata": {},
   "source": [
    "This notebook contains the steps taken in the paper regarding:\n",
    " - Semi-Supervised training for HS detection.\n",
    " - Semi-Supervised training for CS detection.\n",
    " - Topic Modeling of HS comments.\n",
    "\n",
    "All parameters for the different methods can be finetuned as desired."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13192c78-b83e-4e0d-bf19-b031e63b1f16",
   "metadata": {},
   "source": [
    "# Libraries and Mock Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b22a7a-efa1-4eb6-a7f9-f5f767e7f426",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499a86bf-8d09-4aec-a457-58b3d7595dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "# Numerical and Data Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# NLP Libraries\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from textblob import TextBlob\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Machine Learning and Deep Learning\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Transformers and Hugging Face\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    BertTokenizer, BertForSequenceClassification,\n",
    "    TrainingArguments, Trainer, EarlyStoppingCallback,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from llama_cpp import Llama\n",
    "\n",
    "\n",
    "# Topic Modeling and Clustering\n",
    "import hdbscan\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired, LlamaCPP, MaximalMarginalRelevance, PartOfSpeech\n",
    "\n",
    "# Gensim (Topic Coherence)\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Datasets\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "\n",
    "# Initial Setup\n",
    "nltk.download('wordnet')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Environment Variables\n",
    "TF_ENABLE_ONEDNN_OPTS = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d74ab07-5434-4f40-a0fc-7cec628a80be",
   "metadata": {},
   "source": [
    "## Mock Data\n",
    "The following mock data has been generated with ChatGPT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0db9d8-870f-41b5-b5ad-08df4fd77c9c",
   "metadata": {},
   "source": [
    "Mock Data Labeled Datasets HS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca5e38cf-80a3-4ce9-b5f0-74818bd08573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mock_hate_labeled_data():\n",
    "    texts = [\n",
    "        \"This is a terrible comment.\",  # hate\n",
    "        \"You're the worst person ever.\",  # hate\n",
    "        \"I can't stand your opinions.\",  # hate\n",
    "        \"This is hateful and unacceptable.\",  # hate\n",
    "        \"I will report this offensive post.\",  # hate\n",
    "        \"Stop posting such hateful things.\",  # hate\n",
    "        \"This content is deeply offensive.\",  # hate\n",
    "        \"This is pure hate speech.\",  # hate\n",
    "        \"You should be banned for this.\",  # hate\n",
    "        \"This comment is abusive.\",  # hate\n",
    "        \"Great job on the project!\",  # non-hate\n",
    "        \"I agree with this viewpoint.\",  # non-hate\n",
    "        \"This is a very informative post.\",  # non-hate\n",
    "        \"Well done on explaining this topic.\",  # non-hate\n",
    "        \"I support this initiative fully.\",  # non-hate\n",
    "        \"This is a good contribution to the discussion.\",  # non-hate\n",
    "        \"Thank you for sharing this.\",  # non-hate\n",
    "        \"This is a positive and helpful comment.\",  # non-hate\n",
    "        \"Excellent work, keep it up!\",  # non-hate\n",
    "        \"I appreciate your insight on this matter.\"  # non-hate\n",
    "    ]\n",
    "    labels = [1] * 10 + [0] * 10  # First 10 are hate (1), last 10 are non-hate (0)\n",
    "    return pd.DataFrame({'text': texts, 'label': labels})\n",
    "\n",
    "mock_data_hate_labeled = generate_mock_hate_labeled_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a772d7-56ca-48f0-b91d-ce8c6cbab53e",
   "metadata": {},
   "source": [
    "Mock Data Labeled Datasets CS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb39249f-a713-4124-8023-c1bd805d115a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "    'dialogue_id': [\n",
    "        '1001', '1001', '1002', '1002', '1003',\n",
    "        '1003', '1004', '1004', '1005', '1005',\n",
    "        '1006', '1006', '1007', '1007', '1008',\n",
    "        '1008', '1009', '1009', '1010', '1010',\n",
    "        '1011', '1011', '1012', '1012', '1013',\n",
    "        '1013', '1014', '1014', '1015', '1015',\n",
    "        '1016', '1016', '1017', '1017', '1018',\n",
    "        '1018', '1019', '1019', '1020', '1020'\n",
    "    ],\n",
    "    'text': [\n",
    "        \"You're so stupid, I can't believe anyone listens to you.\",  # HS\n",
    "        \"It's important to stay respectful even when we disagree.\",  # CN\n",
    "        \"Why do people like you always spread misinformation?\",  # HS\n",
    "        \"Everyone has different opinions, and that's okay.\",  # CN\n",
    "        \"People like you are ruining the discussion with your hatred.\",  # HS\n",
    "        \"Let's focus on constructive dialogue rather than insults.\",  # CN\n",
    "        \"I find your comments deeply offensive and ignorant.\",  # HS\n",
    "        \"We should be more empathetic and try to understand each other's perspectives.\",  # CN\n",
    "        \"This kind of hate speech should not be tolerated.\",  # HS\n",
    "        \"Respectful conversation is key to resolving conflicts.\",  # CN\n",
    "        \"Your views are so outdated and discriminatory.\",  # HS\n",
    "        \"We can have a meaningful debate without resorting to personal attacks.\",  # CN\n",
    "        \"You only spread negativity and hate.\",  # HS\n",
    "        \"Constructive criticism is a way to improve, not to insult.\",  # CN\n",
    "        \"This rhetoric only fosters division and hatred.\",  # HS\n",
    "        \"Let's engage in discussions that promote mutual respect.\",  # CN\n",
    "        \"Hate speech has no place in our society.\",  # HS\n",
    "        \"Understanding and compassion can bridge gaps between us.\",  # CN\n",
    "        \"Your comment is a perfect example of toxic behavior.\",  # HS\n",
    "        \"We should strive for a more inclusive and respectful dialogue.\",  # CN\n",
    "        \"I can't believe how ignorant some people can be.\",  # HS\n",
    "        \"Engaging in respectful dialogue can help us learn from each other.\",  # CN\n",
    "        \"Your argument is completely unfounded and offensive.\",  # HS\n",
    "        \"Let's focus on finding common ground rather than attacking each other.\",  # CN\n",
    "        \"Your language is very hurtful and damaging.\",  # HS\n",
    "        \"Promoting understanding is essential for constructive discussions.\",  # CN\n",
    "        \"This kind of dialogue only escalates conflict.\",  # HS\n",
    "        \"We should aim for conversations that are both respectful and productive.\",  # CN\n",
    "        \"Your statements are harmful and unnecessary.\",  # HS\n",
    "        \"Open-mindedness and respect are crucial for meaningful exchanges.\",  # CN\n",
    "        \"This kind of negativity is detrimental to productive conversation.\",  # HS\n",
    "        \"Constructive dialogue involves listening and understanding.\",  # CN\n",
    "        \"Your comments only serve to create further division.\",  # HS\n",
    "        \"Let's work towards resolving our differences with mutual respect.\",  # CN\n",
    "        \"This type of rhetoric is unacceptable in any discussion.\",  # HS\n",
    "        \"We should always strive to communicate with empathy and respect.\",  # CN\n",
    "        \"Your behavior is completely unacceptable and damaging.\",  # HS\n",
    "        \"Respectful discourse is the foundation of any healthy discussion.\",  # CN\n",
    "        \"I do not like you.\", # HS\n",
    "        \"You should not answer like this.\" # CN\n",
    "    ],\n",
    "    'turn_id': [\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1\n",
    "    ],\n",
    "    'label': [\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1\n",
    "    ]\n",
    "}\n",
    "\n",
    "mock_dialogues_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818f3522-6cab-4e3c-ad9a-3a9250fc5562",
   "metadata": {},
   "source": [
    "Mock Data TikTok Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3964cd5f-6e6a-428d-bc8b-81bd0c2b5564",
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_data_comments = {\n",
    "    \"cid\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20],\n",
    "    \"text\": [\n",
    "        # Non-hate comments\n",
    "        \"I totally agree with this post!\",\n",
    "        \"Great job on explaining the topic.\",\n",
    "        \"This is such a well-written article, thanks for sharing!\",\n",
    "        \"Your insights are very helpful, I appreciate the effort.\",\n",
    "        \"This is an interesting perspective, and I learned something new.\",\n",
    "        \"I'm really impressed by the clarity of your explanation.\",\n",
    "        \"I couldn't have said it better myself. Well done!\",\n",
    "        \"This was a very informative read, thanks for posting it!\",\n",
    "        \"I really appreciate the thoughtfulness of your comments.\",\n",
    "        \"Such a positive contribution to the discussion!\",\n",
    "        \n",
    "        # Hateful comments\n",
    "        \"You're such an idiot for believing this garbage!\",\n",
    "        \"This is disgusting! People like you make me sick!\",\n",
    "        \"I can't stand how stupid you are, do us all a favor and stop talking.\",\n",
    "        \"Only morons would agree with this nonsense, you must be brain-dead!\",\n",
    "        \"You're a worthless piece of trash, no one cares about your opinion!\",\n",
    "        \"What a waste of time, you're nothing but a pathetic loser!\",\n",
    "        \"It's hard to believe anyone is as dumb as you. Just shut up already.\",\n",
    "        \"You're the reason why the world is so messed up. Go crawl under a rock!\",\n",
    "        \"If ignorance was a crime, you'd be serving life. This is just sad.\",\n",
    "        \"Do everyone a favor and disappear, your existence is pointless!\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "mock_comments_df = pd.DataFrame(mock_data_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216a4eeb-6193-42b0-9447-0019555eac84",
   "metadata": {},
   "source": [
    "Mock Data TikTok Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29b353c9-ec9c-4c3f-bb9b-1899530a7dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = {\n",
    "    'dialogue_id': [\n",
    "        '3001', '3001', '3002', '3002', '3003',\n",
    "        '3003', '3004', '3004', '3005', '3005',\n",
    "        '3006', '3006', '3007', '3007', '3008',\n",
    "        '3008', '3009', '3009', '3010', '3010',\n",
    "        '3011', '3011', '3012', '3012', '3013',\n",
    "        '3013', '3014', '3014', '3015', '3015',\n",
    "        '3016', '3016', '3017', '3017', '3018',\n",
    "        '3018', '3019', '3019', '3020', '3020'\n",
    "    ],\n",
    "    'text': [\n",
    "        \"People like you are ruining this community with your hate.\",  # HS\n",
    "        \"Everyone deserves respect, regardless of their opinions.\",  # CN\n",
    "        \"Your arguments are so backward and offensive.\",  # HS\n",
    "        \"It's important to engage in discussions with empathy and understanding.\",  # CN\n",
    "        \"Why do people like you always spread negativity?\",  # HS\n",
    "        \"Constructive dialogue can bridge gaps and resolve conflicts.\",  # CN\n",
    "        \"Your comment is harmful and discriminatory.\",  # HS\n",
    "        \"Let's focus on positive and respectful communication.\",  # CN\n",
    "        \"You should be ashamed of your prejudiced views.\",  # HS\n",
    "        \"Everyone has different perspectives and that's okay.\",  # CN\n",
    "        \"This kind of rhetoric only perpetuates hatred.\",  # HS\n",
    "        \"Promoting mutual respect and understanding is crucial.\",  # CN\n",
    "        \"Your behavior is unacceptable and damaging.\",  # HS\n",
    "        \"We should strive for more inclusive and respectful conversations.\",  # CN\n",
    "        \"Hate speech has no place in a healthy dialogue.\",  # HS\n",
    "        \"Constructive criticism helps us grow, not tear us down.\",  # CN\n",
    "        \"This kind of negativity only fosters division.\",  # HS\n",
    "        \"Let's engage in discussions that promote empathy and learning.\",  # CN\n",
    "        \"Your language is hurtful and unnecessary.\",  # HS\n",
    "        \"Respectful dialogue is essential for productive conversations.\",  # CN\n",
    "        \"People like you are spreading false information.\",  # HS\n",
    "        \"We can have disagreements without resorting to personal attacks.\",  # CN\n",
    "        \"Your comments are divisive and harmful.\",  # HS\n",
    "        \"Understanding different viewpoints can lead to meaningful discussions.\",  # CN\n",
    "        \"You only contribute to the problem with your hateful speech.\",  # HS\n",
    "        \"Empathy and respect are the foundation of any healthy debate.\",  # CN\n",
    "        \"This kind of discourse is unacceptable and damaging.\",  # HS\n",
    "        \"Promoting kindness and open-mindedness can make a difference.\",  # CN\n",
    "        \"Your views are toxic and unproductive.\",  # HS\n",
    "        \"Let's work towards creating a more respectful and understanding environment.\",  # CN\n",
    "        \"Your comments are inflammatory and disrespectful.\",  # HS\n",
    "        \"Focusing on common ground can help us resolve conflicts.\",  # CN\n",
    "        \"Your attitude is completely unacceptable.\",  # HS\n",
    "        \"We should always strive for constructive and respectful conversations.\",  # CN\n",
    "        \"Your statements are prejudiced and harmful.\",  # HS\n",
    "        \"Encouraging respectful dialogue can lead to positive change.\",  # CN\n",
    "        \"Your argument is completely unfounded and offensive.\",  # HS\n",
    "        \"Let's focus on finding common ground rather than attacking each other.\",  # CN\n",
    "        \"Your language is very hurtful and damaging.\",  # HS\n",
    "        \"Promoting understanding is essential for constructive discussions.\",  # CN\n",
    "    ],\n",
    "    'turn_id': [\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1\n",
    "    ]\n",
    "}\n",
    "\n",
    "mock_dialogues_TikTok_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9245bd53-7e5d-4fb5-8cb3-1ce39690079e",
   "metadata": {},
   "source": [
    "# HS Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f78b63-1dad-4769-87d8-af512feb8236",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b39a9d6-3d81-4fd1-a8ca-603ef57aa2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    if isinstance(text, list):\n",
    "        text = \" \".join(text)\n",
    "    elif not isinstance(text, str):\n",
    "        print(f\"Invalid text input: {text}\")\n",
    "        return \"\"\n",
    "    \n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove usernames, links, hashtags, and punctuation\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)  # Remove usernames\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)  # Remove links\n",
    "    text = re.sub(r\"#(\\w+)\", r\"\\1\", text)  # Remove hashtags\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    \n",
    "    # Correct spelling\n",
    "    # text = str(TextBlob(text).correct())\n",
    "    \n",
    "    # Remove any extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Preprocess comments dataset\n",
    "def preprocess_comments(data):\n",
    "    # Remove rows with missing text or duplicates\n",
    "    data.dropna(subset=['text'], inplace=True)\n",
    "    data = data.drop_duplicates(subset='text')\n",
    "    data['preprocessed_text'] = [preprocess_text(text) for text in tqdm(data['text'], desc=\"Processing Texts\")]\n",
    "    \n",
    "    return data\n",
    "\n",
    "def tokenize_function(data):\n",
    "    # Ensure that 'text' field is present and tokenize\n",
    "    return tokenizer(data['text'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "def predict_hate(dataset, model, tokenizer):\n",
    "    # Create a DataLoader with batch size 16 and padding using the tokenizer\n",
    "    loader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=16, \n",
    "        collate_fn=DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    )\n",
    "    \n",
    "    # Prepare lists to store texts and predictions\n",
    "    texts, probabilities = [], []\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Iterate over batches\n",
    "        for batch in loader:\n",
    "            # Get model outputs\n",
    "            outputs = model(batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "            # Convert logits to probabilities using softmax\n",
    "            probs = torch.softmax(outputs.logits, dim=1)\n",
    "            \n",
    "            # Store text and probabilities\n",
    "            for i in range(batch['input_ids'].size(0)):\n",
    "                texts.append(tokenizer.decode(batch['input_ids'][i], skip_special_tokens=True))\n",
    "                probabilities.append(probs[i].tolist())\n",
    "\n",
    "    # Return results as a DataFrame\n",
    "    return pd.DataFrame({\n",
    "        'text': texts,\n",
    "        'non_hate_probability': [p[0] for p in probabilities],  # Probability for non-hate speech\n",
    "        'hate_probability': [p[1] for p in probabilities]   # Probability for hate speech\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d1a944-8fcb-47f4-aae2-45bdec77f3ac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Training Model with labeled data from literature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58743c1-f015-4667-9f1a-2bfdf2fc45aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_data_hate_labeled = preprocess_comments(mock_data_hate_labeled)\n",
    "mock_dataset_hate_labeled = Dataset.from_pandas(mock_data_hate_labeled)\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"GroNLP/hateBERT\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"GroNLP/hateBERT\", num_labels=2)\n",
    "\n",
    "\n",
    "# Split the dataset into train/test/validation with StratifiedShuffleSplit\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.4, random_state=42)\n",
    "train_indices, temp_indices = next(splitter.split(mock_dataset_hate_labeled['label'], mock_dataset_hate_labeled['label']))\n",
    "train_set = mock_dataset_hate_labeled.select(train_indices)\n",
    "temp_set = mock_dataset_hate_labeled.select(temp_indices)\n",
    "\n",
    "# Further split temp into validation and test sets\n",
    "temp_labels = temp_set['label']\n",
    "splitter_temp = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "validation_indices, test_indices = next(splitter_temp.split(temp_set['label'], temp_labels))\n",
    "validation_set = temp_set.select(validation_indices)\n",
    "test_set = temp_set.select(test_indices)\n",
    "\n",
    "\n",
    "# Tokenize the datasets\n",
    "tokenized_train = train_set.map(tokenize_function, batched=True)\n",
    "tokenized_validation = validation_set.map(tokenize_function, batched=True)\n",
    "tokenized_test = test_set.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bfef60-23a8-47e1-a5b6-bd08c7a6a219",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to create and train a model with given hyperparameters\n",
    "def train_model_with_params(learning_rate, batch_size, num_epochs):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=500,\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=num_epochs,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=100,\n",
    "        save_total_limit=1,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_validation,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    \n",
    "    # Evaluate the model on the validation set\n",
    "    eval_results = trainer.evaluate()\n",
    "    eval_loss = eval_results[\"eval_loss\"]\n",
    "    \n",
    "    return eval_loss\n",
    "\n",
    "# Define hyperparameter grid as desired\n",
    "hyperparameter_grid = {\n",
    "    'learning_rate': [1e-5, 3e-5, 5e-5],\n",
    "    'per_device_train_batch_size': [8, 16],\n",
    "    'num_train_epochs': [3, 5]\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "best_params = None\n",
    "best_loss = np.inf\n",
    "\n",
    "for lr in hyperparameter_grid['learning_rate']:\n",
    "    for batch_size in hyperparameter_grid['per_device_train_batch_size']:\n",
    "        for num_epochs in hyperparameter_grid['num_train_epochs']:\n",
    "            print(f\"Training with lr={lr}, batch_size={batch_size}, num_epochs={num_epochs}\")\n",
    "            loss = train_model_with_params(lr, batch_size, num_epochs)\n",
    "            print(f\"Validation Evaluation Loss: {loss}\")\n",
    "            \n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                best_params = (lr, batch_size, num_epochs)\n",
    "\n",
    "print(f\"Best Hyperparameters: learning_rate={best_params[0]}, batch_size={best_params[1]}, num_epochs={best_params[2]}\")\n",
    "\n",
    "# Train final model with the best hyperparameters on the full training data\n",
    "best_learning_rate, best_batch_size, best_num_epochs = best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac455b13-b355-4cd0-bcaa-0afe94253916",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    learning_rate=best_learning_rate,\n",
    "    per_device_train_batch_size=best_batch_size,\n",
    "    per_device_eval_batch_size=best_batch_size,\n",
    "    num_train_epochs=best_num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False\n",
    ")\n",
    "\n",
    "final_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=final_training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_validation,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "final_trainer.train()\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"./SupervisedTrainingHateBERT\")\n",
    "tokenizer.save_pretrained(\"./SupervisedTrainingHateBERT\")\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_results = final_trainer.evaluate(eval_dataset=tokenized_test)\n",
    "test_loss = test_results[\"eval_loss\"]\n",
    "\n",
    "print(f\"Test Set Evaluation Loss: {test_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33726566-05d4-47a5-9644-87f2b99d6dbe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Predicting TikTok messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cd5039-5677-45d0-8848-58df60403b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model for hate speech detection\n",
    "tokenizer_hate = AutoTokenizer.from_pretrained(\"./SupervisedTrainingHateBERT\")\n",
    "model_hate = AutoModelForSequenceClassification.from_pretrained(\"./SupervisedTrainingHateBERT\")\n",
    "\n",
    "# Preprocess comments and get the preprocessed text\n",
    "comments_data = preprocess_comments(mock_comments_df)\n",
    "preprocessed_texts = comments_data['preprocessed_text'].to_list()\n",
    "\n",
    "# Tokenize preprocessed texts using the hate speech tokenizer\n",
    "encodings_hate = tokenizer_hate(preprocessed_texts, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "# Create dataset for hate speech predictions\n",
    "dataset_hate = Dataset.from_dict({\n",
    "    'input_ids': encodings_hate['input_ids'],\n",
    "    'attention_mask': encodings_hate['attention_mask']\n",
    "})\n",
    "\n",
    "# Make predictions\n",
    "predictions_df = predict_hate(dataset_hate, model_hate, tokenizer_hate)\n",
    "\n",
    "# You can then save the DataFrame to a CSV file if needed\n",
    "predictions_file = \"hateFirstTraining.csv\"\n",
    "predictions_df.to_csv(predictions_file, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc20c73-348e-4931-b9bc-5a1b6ae4e397",
   "metadata": {},
   "source": [
    "## Obtain pseudolabels and retraining HS prediction model with labels and pseudo-labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d954ec4e-111d-40a0-b8ba-fdda08e5fe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pseudolabels\n",
    "hate_comments_predictions = pd.read_csv(\"hateFirstTraining.csv\")\n",
    "\n",
    "# Variables to control the pseudolabeling process\n",
    "hate_probability_threshold_pseudolabel = False\n",
    "non_hate_probability_threshold_pseudolabel = False\n",
    "\n",
    "# Hate pseudolabeling based on probability threshold or quantile\n",
    "if hate_probability_threshold_pseudolabel:\n",
    "    probability_threshold = 0.95\n",
    "    # Only keep rows with hate probability greater than the threshold\n",
    "    hate_pseudolabels = hate_comments_predictions[hate_comments_predictions[\"hate_probability\"] > probability_threshold]\n",
    "    hate_pseudolabels[\"label\"] = 1\n",
    "else:\n",
    "    quantile = 0.05\n",
    "    # Get the top 5% of hate comments based on the hate probability\n",
    "    top_5_percent_cutoff = hate_comments_predictions[\"hate_probability\"].quantile(1 - quantile)\n",
    "    hate_pseudolabels = hate_comments_predictions[hate_comments_predictions[\"hate_probability\"] >= top_5_percent_cutoff]\n",
    "    hate_pseudolabels[\"label\"] = 1\n",
    "\n",
    "# Non-hate pseudolabeling based on probability threshold or quantile\n",
    "if non_hate_probability_threshold_pseudolabel:\n",
    "    probability_threshold = 0.05\n",
    "    # Only keep rows with hate probability less than the threshold\n",
    "    non_hate_pseudolabels = hate_comments_predictions[hate_comments_predictions[\"hate_probability\"] < probability_threshold]\n",
    "    non_hate_pseudolabels[\"label\"] = 0\n",
    "else:\n",
    "    quantile = 0.05\n",
    "    # Get the bottom 5% of comments based on the hate probability\n",
    "    bottom_5_percent_cutoff = hate_comments_predictions[\"hate_probability\"].quantile(quantile)\n",
    "    non_hate_pseudolabels = hate_comments_predictions[hate_comments_predictions[\"hate_probability\"] <= bottom_5_percent_cutoff]\n",
    "    non_hate_pseudolabels[\"label\"] = 0\n",
    "\n",
    "# Combine the hate and non-hate pseudolabels\n",
    "hate_pseudolabels = pd.concat([hate_pseudolabels, non_hate_pseudolabels], ignore_index=True)\n",
    "\n",
    "# Display the final pseudolabels dataset\n",
    "hate_pseudolabels = hate_pseudolabels[[\"text\", \"label\"]]\n",
    "hate_pseudolabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778b0af8-829d-4fe6-98cc-777643d6db20",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"GroNLP/hateBERT\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"GroNLP/hateBERT\", num_labels=2)\n",
    "\n",
    "# Merge the DataFrames\n",
    "hate_dataset = pd.concat([mock_data_hate_labeled, hate_pseudolabels])\n",
    "\n",
    "hate_dataset = Dataset.from_pandas(hate_dataset)\n",
    "\n",
    "# Split the dataset into train/test/validation with StratifiedShuffleSplit\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_indices, temp_indices = next(splitter.split(hate_dataset['label'], hate_dataset['label']))\n",
    "train_set = hate_dataset.select(train_indices)\n",
    "temp_set = hate_dataset.select(temp_indices)\n",
    "\n",
    "# Further split temp into validation and test sets\n",
    "temp_labels = temp_set['label']\n",
    "splitter_temp = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "validation_indices, test_indices = next(splitter_temp.split(temp_set['label'], temp_labels))\n",
    "validation_set = temp_set.select(validation_indices)\n",
    "test_set = temp_set.select(test_indices)\n",
    "\n",
    "# Tokenization function for dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Tokenize the datasets\n",
    "tokenized_train = train_set.map(tokenize_function, batched=True)\n",
    "tokenized_validation = validation_set.map(tokenize_function, batched=True)\n",
    "tokenized_test = test_set.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e205ce3-b827-4e29-852f-6afece7590f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grid\n",
    "hyperparameter_grid = {\n",
    "    'learning_rate': [1e-5, 3e-5, 5e-5],\n",
    "    'per_device_train_batch_size': [8, 16],\n",
    "    'num_train_epochs': [3, 5]\n",
    "}\n",
    "\n",
    "# Function to create and train a model with given hyperparameters\n",
    "def train_and_evaluate(learning_rate, batch_size, num_epochs):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=500,\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=num_epochs,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=100,\n",
    "        save_total_limit=1,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_validation,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    \n",
    "    # Evaluate on the test set\n",
    "    test_results = trainer.evaluate(tokenized_test)\n",
    "    eval_loss = test_results[\"eval_loss\"]\n",
    "    \n",
    "    return eval_loss\n",
    "\n",
    "# Grid search\n",
    "best_params = None\n",
    "best_loss = np.inf\n",
    "\n",
    "for lr in hyperparameter_grid['learning_rate']:\n",
    "    for batch_size in hyperparameter_grid['per_device_train_batch_size']:\n",
    "        for num_epochs in hyperparameter_grid['num_train_epochs']:\n",
    "            print(f\"Training with lr={lr}, batch_size={batch_size}, num_epochs={num_epochs}\")\n",
    "            loss = train_and_evaluate(lr, batch_size, num_epochs)\n",
    "            print(f\"Evaluation Loss: {loss}\")\n",
    "            \n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                best_params = (lr, batch_size, num_epochs)\n",
    "\n",
    "print(f\"Best Parameters: learning_rate={best_params[0]}, batch_size={best_params[1]}, num_epochs={best_params[2]}\")\n",
    "\n",
    "# Use the best hyperparameters to train and save the final model\n",
    "best_lr, best_batch_size, best_num_epochs = best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b251e06-1a08-461d-a3af-cef189848063",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    learning_rate=best_lr,\n",
    "    per_device_train_batch_size=best_batch_size,\n",
    "    per_device_eval_batch_size=best_batch_size,\n",
    "    num_train_epochs=best_num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False\n",
    ")\n",
    "\n",
    "final_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=final_training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_validation,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "final_trainer.train()\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_results = final_trainer.evaluate(tokenized_test)\n",
    "print(\"Test dataset results:\", test_results)\n",
    "\n",
    "# Save the final model and tokenizer\n",
    "model.save_pretrained(\"./Semi_SupervisedTrainingHateBERT\")\n",
    "tokenizer.save_pretrained(\"./Semi_SupervisedTrainingHateBERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c32b95-0712-4b29-a392-161993c43853",
   "metadata": {},
   "source": [
    "## Predicting final HS probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07190636-3950-498a-a870-b5ab2d2ff1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load resources\n",
    "tokenizer_hate = AutoTokenizer.from_pretrained(\"./Semi_SupervisedTrainingHateBERT\")\n",
    "model_hate = AutoModelForSequenceClassification.from_pretrained(\"./Semi_SupervisedTrainingHateBERT\")\n",
    "\n",
    "# Preprocess comments and get the preprocessed text\n",
    "comments_data = preprocess_comments(mock_comments_df)\n",
    "preprocessed_texts = comments_data['preprocessed_text'].to_list()\n",
    "\n",
    "# Tokenize preprocessed texts using the hate speech tokenizer\n",
    "encodings_hate = tokenizer_hate(preprocessed_texts, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "# Create dataset for hate speech predictions\n",
    "dataset_hate = Dataset.from_dict({\n",
    "    'input_ids': encodings_hate['input_ids'],\n",
    "    'attention_mask': encodings_hate['attention_mask']\n",
    "})\n",
    "\n",
    "# Make predictions\n",
    "predictions_df = predict_hate(dataset_hate, model_hate, tokenizer_hate)\n",
    "\n",
    "# You can then save the DataFrame to a CSV file if needed\n",
    "predictions_file = \"finalHatePredictions.csv\"\n",
    "predictions_df.to_csv(predictions_file, index=False)\n",
    "predictions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867770c2-34d7-430a-81d9-94a66d0768e3",
   "metadata": {},
   "source": [
    "# CS Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c15e47d-1efa-4284-a952-5cc4c0440c04",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e9bd7db-b8d1-4833-bb3e-47609b810981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the function to prepare data\n",
    "def prepare_data(df):\n",
    "    pairs, labels = [], []\n",
    "    grouped = df.groupby('dialogue_id')\n",
    "    for _, group in grouped:\n",
    "        if len(group) % 2 != 0:\n",
    "            group = group[:-1]\n",
    "        \n",
    "        for i in range(0, len(group), 2):\n",
    "            combined_text = \"[CLS] \" + preprocess_text(group.iloc[i]['text']) + \" [SEP] \" + preprocess_text(group.iloc[i+1]['text'])\n",
    "            combined_label = [group.iloc[i]['label'], group.iloc[i+1]['label']]\n",
    "            pairs.append(combined_text)\n",
    "            labels.append(combined_label)\n",
    "    return pd.DataFrame({'text': pairs, 'labels': labels})\n",
    "\n",
    "# Define the function to prepare data\n",
    "def prepare_data_TikTok(df):\n",
    "    pairs = []\n",
    "    grouped = df.groupby('dialogue_id')\n",
    "    for _, group in grouped:\n",
    "        if len(group) % 2 != 0:\n",
    "            group = group[:-1]\n",
    "        \n",
    "        for i in range(0, len(group), 2):\n",
    "            combined_text = \"[CLS] \" + preprocess_text(group.iloc[i]['text']) + \" [SEP] \" + preprocess_text(group.iloc[i+1]['text'])\n",
    "            pairs.append(combined_text)\n",
    "    return pd.DataFrame({'text': pairs})\n",
    "    \n",
    "# Split data\n",
    "def split_data(df):\n",
    "    X = df['text']\n",
    "    y = df['labels']\n",
    "    \n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "    X_validation, X_test, y_validation, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "    \n",
    "    return X_train, X_validation, X_test, y_train, y_validation, y_test\n",
    "\n",
    "# Define the Dataset class\n",
    "class DialogueDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=128):\n",
    "        self.texts = df['text'].tolist()\n",
    "        self.labels = [torch.tensor(label, dtype=torch.float32) for label in df['labels']]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        labels = self.labels[idx]\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'labels': labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0345ea39-639f-4dc4-a20e-dc6e754d070d",
   "metadata": {},
   "source": [
    "## Training Model with labeled data from literature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8456bd-354a-4681-a431-50a32cb862a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "import pandas as pd\n",
    "\n",
    "# Define a function to create and train a model with given hyperparameters\n",
    "def train_model_with_params(learning_rate, batch_size, num_epochs):\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=500,\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=num_epochs,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=100,\n",
    "        save_total_limit=1,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False\n",
    "    )\n",
    "\n",
    "    # Initialize the model\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "    # Define the trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=validation_dataset,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    \n",
    "    # Evaluate the model on the validation set\n",
    "    eval_results = trainer.evaluate()\n",
    "    eval_loss = eval_results[\"eval_loss\"]\n",
    "    \n",
    "    return eval_loss\n",
    "\n",
    "# Prepare and split data\n",
    "pairs_df = prepare_data(mock_dialogues_df)\n",
    "X_train, X_validation, X_test, y_train, y_validation, y_test = split_data(pairs_df)\n",
    "\n",
    "# Convert data to DataFrame\n",
    "train_df = pd.DataFrame({'text': X_train, 'labels': y_train.tolist()})\n",
    "validation_df = pd.DataFrame({'text': X_validation, 'labels': y_validation.tolist()})\n",
    "test_df = pd.DataFrame({'text': X_test, 'labels': y_test.tolist()})\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = DialogueDataset(train_df, tokenizer)\n",
    "validation_dataset = DialogueDataset(validation_df, tokenizer)\n",
    "test_dataset = DialogueDataset(test_df, tokenizer)\n",
    "\n",
    "# Define hyperparameter grid as desired\n",
    "hyperparameter_grid = {\n",
    "    'learning_rate': [1e-5, 3e-5, 5e-5],\n",
    "    'per_device_train_batch_size': [8, 16],\n",
    "    'num_train_epochs': [3, 5]\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "best_params = None\n",
    "best_loss = np.inf\n",
    "\n",
    "for lr in hyperparameter_grid['learning_rate']:\n",
    "    for batch_size in hyperparameter_grid['per_device_train_batch_size']:\n",
    "        for num_epochs in hyperparameter_grid['num_train_epochs']:\n",
    "            print(f\"Training with lr={lr}, batch_size={batch_size}, num_epochs={num_epochs}\")\n",
    "            loss = train_model_with_params(lr, batch_size, num_epochs)\n",
    "            print(f\"Validation Evaluation Loss: {loss}\")\n",
    "            \n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                best_params = (lr, batch_size, num_epochs)\n",
    "\n",
    "print(f\"Best Hyperparameters: learning_rate={best_params[0]}, batch_size={best_params[1]}, num_epochs={best_params[2]}\")\n",
    "\n",
    "# Retrain the model with the best hyperparameters on the full training data\n",
    "best_learning_rate, best_batch_size, best_num_epochs = best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4ffe97-dd8a-4787-879d-a1e03fa3ccb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments with best hyperparameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    learning_rate=best_learning_rate,\n",
    "    per_device_train_batch_size=best_batch_size,\n",
    "    per_device_eval_batch_size=best_batch_size,\n",
    "    num_train_epochs=best_num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False\n",
    ")\n",
    "\n",
    "# Initialize and train the final model with the best parameters\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# Save the model and tokenizer\n",
    "import os\n",
    "os.makedirs(\"./SupervisedTrainingDialogueBERT\", exist_ok=True)\n",
    "model.save_pretrained(\"./SupervisedTrainingDialogueBERT\")\n",
    "tokenizer.save_pretrained(\"./SupervisedTrainingDialogueBERT\")\n",
    "\n",
    "# Evaluate on the test dataset\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "print(\"Test dataset results:\", test_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712b3e68-f3cd-4549-a5e2-2fd5fce26356",
   "metadata": {},
   "source": [
    "## Predicting TikTok Dialogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48817f58-4190-4443-8d0d-4dd7fc3c9ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./SupervisedTrainingDialogueBERT\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"./SupervisedTrainingDialogueBERT\")\n",
    "\n",
    "class PredictionDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=128):\n",
    "        self.texts = df['text'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten()\n",
    "        }\n",
    "\n",
    "pairs_df_TikTok = prepare_data_TikTok(mock_dialogues_TikTok_df)\n",
    "# Create the dataset\n",
    "prediction_dataset = PredictionDataset(pairs_df_TikTok, tokenizer)\n",
    "\n",
    "# Create a dataloader\n",
    "dataloader = DataLoader(prediction_dataset, batch_size=8)\n",
    "\n",
    "def predict(dataloader, model):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            probs = torch.nn.functional.softmax(logits, dim=1).cpu().numpy()\n",
    "            \n",
    "            all_preds.extend(probs)\n",
    "    return all_preds\n",
    "\n",
    "# Get predictions\n",
    "probs = predict(dataloader, model)\n",
    "\n",
    "pairs_df_TikTok[\"probabilities\"] = probs\n",
    "print(\"Probabilities:\\n\", probs)\n",
    "print(len(probs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5df734-be46-4ebf-9c4f-808978d20cbc",
   "metadata": {},
   "source": [
    "## Obtain pseudolabels and retraining CS prediction model with labels and pseudo-labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97f04a4-ce81-452e-aff6-2c44329d72ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_df_TikTok[['probability_1', 'probability_2']] = pd.DataFrame(pairs_df_TikTok['probabilities'].tolist(), index=pairs_df_TikTok.index)\n",
    "\n",
    "# Variables to control the pseudolabeling process\n",
    "counter_speech_probability_threshold_pseudolabel = False\n",
    "non_counter_speech_probability_threshold_pseudolabel = False\n",
    "\n",
    "def assign_labels(df, threshold=None, quantile=None, is_counter_speech=True):\n",
    "    labels = []\n",
    "    for _, row in df.iterrows():\n",
    "        prob = row[['probability_1', 'probability_2']].values\n",
    "        max_prob = max(prob)\n",
    "        index_max_prob = prob.argmax()\n",
    "        \n",
    "        if threshold is not None:\n",
    "            if is_counter_speech:\n",
    "                label = [0, 1] if prob[1] > threshold else [1, 0]\n",
    "            else:\n",
    "                label = [1, 0] if prob[1] < (1 - threshold) else [0, 1]\n",
    "        else:\n",
    "            if is_counter_speech:\n",
    "                cutoff = pd.Series(prob).quantile(1 - quantile)\n",
    "                label = [0, 1] if max_prob >= cutoff else [1, 0]\n",
    "            else:\n",
    "                cutoff = pd.Series(prob).quantile(quantile)\n",
    "                label = [1, 0] if min(prob) <= cutoff else [0, 1]\n",
    "        \n",
    "        labels.append(label)\n",
    "    return labels\n",
    "\n",
    "# Pseudolabeling for counter speech\n",
    "if counter_speech_probability_threshold_pseudolabel:\n",
    "    probability_threshold = 0.95\n",
    "    counter_speech_pseudolabels = pairs_df_TikTok[pairs_df_TikTok[['probability_1', 'probability_2']].max(axis=1) > probability_threshold]\n",
    "    counter_speech_pseudolabels['labels'] = assign_labels(counter_speech_pseudolabels, threshold=probability_threshold, is_counter_speech=True)\n",
    "else:\n",
    "    quantile = 0.05\n",
    "    top_5_percent_cutoff = pairs_df_TikTok[['probability_1', 'probability_2']].max(axis=1).quantile(1 - quantile)\n",
    "    counter_speech_pseudolabels = pairs_df_TikTok[pairs_df_TikTok[['probability_1', 'probability_2']].max(axis=1) >= top_5_percent_cutoff]\n",
    "    counter_speech_pseudolabels['labels'] = assign_labels(counter_speech_pseudolabels, quantile=quantile, is_counter_speech=True)\n",
    "\n",
    "# Display the final DataFrame with pseudolabels\n",
    "counter_speech_pseudolabels = counter_speech_pseudolabels[[\"text\", \"labels\"]]\n",
    "print(counter_speech_pseudolabels)\n",
    "\n",
    "pairs_df_labeled = prepare_data(mock_dialogues_df)\n",
    "pairs_df = pd.concat([pairs_df_labeled, counter_speech_pseudolabels])\n",
    "pairs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ea5c05-e030-4e2c-bd1e-5022cd58e264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "import pandas as pd\n",
    "\n",
    "# Define a function to create and train a model with given hyperparameters\n",
    "def train_model_with_params(learning_rate, batch_size, num_epochs):\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=500,\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=num_epochs,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=100,\n",
    "        save_total_limit=1,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False\n",
    "    )\n",
    "\n",
    "    # Initialize the model\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "    # Define the trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=validation_dataset,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    \n",
    "    # Evaluate the model on the validation set\n",
    "    eval_results = trainer.evaluate()\n",
    "    eval_loss = eval_results[\"eval_loss\"]\n",
    "    \n",
    "    return eval_loss\n",
    "\n",
    "# Prepare and split data\n",
    "pairs_df = prepare_data(mock_dialogues_df)\n",
    "X_train, X_validation, X_test, y_train, y_validation, y_test = split_data(pairs_df)\n",
    "\n",
    "# Convert data to DataFrame\n",
    "train_df = pd.DataFrame({'text': X_train, 'labels': y_train.tolist()})\n",
    "validation_df = pd.DataFrame({'text': X_validation, 'labels': y_validation.tolist()})\n",
    "test_df = pd.DataFrame({'text': X_test, 'labels': y_test.tolist()})\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = DialogueDataset(train_df, tokenizer)\n",
    "validation_dataset = DialogueDataset(validation_df, tokenizer)\n",
    "test_dataset = DialogueDataset(test_df, tokenizer)\n",
    "\n",
    "# Define hyperparameter grid as desired\n",
    "hyperparameter_grid = {\n",
    "    'learning_rate': [1e-5, 3e-5, 5e-5],\n",
    "    'per_device_train_batch_size': [8, 16],\n",
    "    'num_train_epochs': [3, 5]\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "best_params = None\n",
    "best_loss = np.inf\n",
    "\n",
    "for lr in hyperparameter_grid['learning_rate']:\n",
    "    for batch_size in hyperparameter_grid['per_device_train_batch_size']:\n",
    "        for num_epochs in hyperparameter_grid['num_train_epochs']:\n",
    "            print(f\"Training with lr={lr}, batch_size={batch_size}, num_epochs={num_epochs}\")\n",
    "            loss = train_model_with_params(lr, batch_size, num_epochs)\n",
    "            print(f\"Validation Evaluation Loss: {loss}\")\n",
    "            \n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                best_params = (lr, batch_size, num_epochs)\n",
    "\n",
    "print(f\"Best Hyperparameters: learning_rate={best_params[0]}, batch_size={best_params[1]}, num_epochs={best_params[2]}\")\n",
    "\n",
    "# Retrain the model with the best hyperparameters on the full training data\n",
    "best_learning_rate, best_batch_size, best_num_epochs = best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dab131a-8d07-4bc3-9714-8269a8b058d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments with best hyperparameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    learning_rate=best_learning_rate,\n",
    "    per_device_train_batch_size=best_batch_size,\n",
    "    per_device_eval_batch_size=best_batch_size,\n",
    "    num_train_epochs=best_num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False\n",
    ")\n",
    "\n",
    "# Initialize and train the final model with the best parameters\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# Save the model and tokenizer\n",
    "import os\n",
    "os.makedirs(\"./SemiSupervisedTrainingDialogueBERT\", exist_ok=True)\n",
    "model.save_pretrained(\"./SemiSupervisedTrainingDialogueBERT\")\n",
    "tokenizer.save_pretrained(\"./SemiSupervisedTrainingDialogueBERT\")\n",
    "\n",
    "# Evaluate on the test dataset\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "print(\"Test dataset results:\", test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4e551b-795d-4929-82e6-a70e9f40bd87",
   "metadata": {},
   "source": [
    "## Predicting final CS probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37feafb0-ca68-47e5-9b01-23dd9b8c0fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./SemiSupervisedTrainingDialogueBERT\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"./SemiSupervisedTrainingDialogueBERT\")\n",
    "\n",
    "class PredictionDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=128):\n",
    "        self.texts = df['text'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten()\n",
    "        }\n",
    "\n",
    "pairs_df_TikTok = prepare_data_TikTok(mock_dialogues_TikTok_df)\n",
    "# Create the dataset\n",
    "prediction_dataset = PredictionDataset(pairs_df_TikTok, tokenizer)\n",
    "\n",
    "# Create a dataloader\n",
    "dataloader = DataLoader(prediction_dataset, batch_size=8)\n",
    "\n",
    "def predict(dataloader, model):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            probs = torch.nn.functional.softmax(logits, dim=1).cpu().numpy()\n",
    "            \n",
    "            all_preds.extend(probs)\n",
    "    return all_preds\n",
    "\n",
    "# Get predictions\n",
    "probs = predict(dataloader, model)\n",
    "\n",
    "pairs_df_TikTok[\"probabilities\"] = probs\n",
    "print(\"Probabilities:\\n\", probs)\n",
    "print(len(probs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b591ee98-10ae-47cd-8c0a-512714b363c7",
   "metadata": {},
   "source": [
    "# HS Comments Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564c279a-43b1-452f-b0ee-1423d77fa285",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b7f0cab7-5fbb-4ca3-ad28-bb5b101097f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tqdm pandas extension\n",
    "def clean_text(text):\n",
    "    # Initialize the lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Remove mentions, URLs, and hashtags\n",
    "    text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'#', '', text)  # Remove the # symbol but keep the text following it\n",
    "\n",
    "    # Define emoticon pattern\n",
    "    emoticon_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # Symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # Transport & map symbols\n",
    "        u\"\\U0001F700-\\U0001F77F\"  # Alchemical symbols\n",
    "        u\"\\U0001F780-\\U0001F7FF\"  # Geometric shapes extended\n",
    "        u\"\\U0001F800-\\U0001F8FF\"  # Supplemental arrows-C\n",
    "        u\"\\U0001F900-\\U0001F9FF\"  # Supplemental symbols and pictographs\n",
    "        u\"\\U0001FA00-\\U0001FA6F\"  # Chess symbols\n",
    "        u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and pictographs extended-A\n",
    "        u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "        u\"\\U000024C2-\\U0001F251\"  # Enclosed characters\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    # Remove emoticons\n",
    "    text = emoticon_pattern.sub(r'', text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def calculate_coherence_score(topic_model, docs, dictionary):\n",
    "    topics = topic_model.get_topics()\n",
    "\n",
    "    topic_list = [[word for word, _ in topic] for topic in topics.values()]\n",
    "    \n",
    "    texts = [doc.split() for doc in docs]\n",
    "    \n",
    "    dictionary = Dictionary(texts)\n",
    "\n",
    "    coherence_model = CoherenceModel(topics=topic_list, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "    return coherence_model.get_coherence()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56dcc47-b31b-43c5-af62-8f131ba0f65f",
   "metadata": {},
   "source": [
    "## Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "37e1fe0d-1e43-4dae-8e6f-5e4323ca5a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "commentsData = pd.read_csv(\"finalHatePredictions.csv\")\n",
    "commentsData[\"is_hate_speech\"] = (commentsData[\"hate_probability\"] > 0.5).astype(int)\n",
    "hateMessages = commentsData[commentsData['is_hate_speech'] == 1]\n",
    "hateMessages['clean_text'] = hateMessages['text'].apply(lambda x: clean_text(x))\n",
    "hateMessages = hateMessages[hateMessages['clean_text'].str.strip().astype(bool)]\n",
    "\n",
    "hate_speech_docs = hateMessages['clean_text'].tolist()\n",
    "embedding_model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "embeddings = embedding_model.encode(hate_speech_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648086c9-620d-40bf-9e06-6dbcbcb4a7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "representation_model = {\n",
    "    \"KeyBERT\": KeyBERTInspired(),\n",
    "    \"POS\": PartOfSpeech(\"en_core_web_trf\"),\n",
    "    \"KeyBERT_MMR\": [KeyBERTInspired(top_n_words=10), MaximalMarginalRelevance(diversity=.8)],\n",
    "}\n",
    "\n",
    "# Define a range of parameters to test\n",
    "vectorizer_params = [\n",
    "    {'stop_words': 'english', 'ngram_range': (1, 1)},\n",
    "    # Add more parameter configurations here\n",
    "]\n",
    "\n",
    "hdbscan_params = [\n",
    "    {'min_cluster_size': 2, 'metric': 'euclidean', 'cluster_selection_method': 'eom', 'prediction_data': True},\n",
    "    {'min_cluster_size': 2, 'metric': 'manhattan', 'cluster_selection_method': 'leaf', 'prediction_data': True},\n",
    "    # Add more parameter configurations here\n",
    "]\n",
    "\n",
    "best_coherence = -float('inf')\n",
    "best_params = {}\n",
    "\n",
    "# Convert documents to dictionary format for coherence calculation\n",
    "dictionary = Dictionary([doc.split() for doc in hate_speech_docs])\n",
    "\n",
    "for vectorizer_param in vectorizer_params:\n",
    "    for hdbscan_param in hdbscan_params:\n",
    "        vectorizer_model = CountVectorizer(**vectorizer_param)\n",
    "        hdbscan_model = hdbscan.HDBSCAN(**hdbscan_param)\n",
    "\n",
    "        topic_model = BERTopic(\n",
    "            language=\"english\",\n",
    "            embedding_model=embedding_model,\n",
    "            vectorizer_model=vectorizer_model,\n",
    "            representation_model=representation_model,\n",
    "            verbose=True,\n",
    "            calculate_probabilities=True,\n",
    "            hdbscan_model=hdbscan_model\n",
    "        )\n",
    "        \n",
    "        topics, probs = topic_model.fit_transform(hate_speech_docs, embeddings)\n",
    "        \n",
    "        coherence_score = calculate_coherence_score(topic_model, hate_speech_docs, dictionary)\n",
    "        \n",
    "        if coherence_score > best_coherence:\n",
    "            best_coherence = coherence_score\n",
    "            best_params = {\n",
    "                'vectorizer': vectorizer_param,\n",
    "                'hdbscan': hdbscan_param\n",
    "            }\n",
    "\n",
    "# Print the best parameters and their coherence score\n",
    "print(\"Best Coherence Score:\", best_coherence)\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Final model with best parameters\n",
    "best_vectorizer_model = CountVectorizer(**best_params['vectorizer'])\n",
    "best_hdbscan_model = hdbscan.HDBSCAN(**best_params['hdbscan'])\n",
    "\n",
    "final_topic_model = BERTopic(\n",
    "    language=\"english\",\n",
    "    embedding_model=embedding_model,\n",
    "    vectorizer_model=best_vectorizer_model,\n",
    "    representation_model=representation_model,\n",
    "    top_n_words=10,\n",
    "    verbose=True,\n",
    "    calculate_probabilities=True,\n",
    "    hdbscan_model=best_hdbscan_model\n",
    ")\n",
    "\n",
    "# Assuming hate_speech_docs and embeddings are already defined\n",
    "topics, probs = final_topic_model.fit_transform(hate_speech_docs, embeddings)\n",
    "hateMessages['Topic'] = topics\n",
    "topic_info = final_topic_model.get_topic_info()\n",
    "\n",
    "hateMessagesWithTopicInfo = pd.merge(\n",
    "    hateMessages, \n",
    "    topic_info[['Topic', 'Representation', 'KeyBERT', 'POS', 'KeyBERT_MMR', 'Representative_Docs']], \n",
    "    on='Topic', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "hateMessagesWithTopicInfo.to_csv('hateMessagesWithTopicInformation.csv', index=False)\n",
    "hateMessagesWithTopicInfo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a000a269-832e-4b6d-8f16-6d945e81accc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the topic_model\n",
    "with open('topic_model_hate_comments.pkl', 'wb') as model_file:\n",
    "    pickle.dump(topic_model, model_file)\n",
    "\n",
    "# Save the topics\n",
    "with open('topics_hate_comments.pkl', 'wb') as topics_file:\n",
    "    pickle.dump(topics, topics_file)\n",
    "\n",
    "# Save the probabilities\n",
    "with open('probs_hate_comments.pkl', 'wb') as probs_file:\n",
    "    pickle.dump(probs, probs_file)\n",
    "\n",
    "print(\"Topic model, topics, and probabilities have been saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab2cdb4-6a30-4d11-9de1-c5acf2c1e357",
   "metadata": {},
   "source": [
    "## Labeling Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2e8742-d5b8-4101-9f7a-32d98c5febea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the topic_model\n",
    "with open('topic_model_hate_comments.pkl', 'rb') as model_file:\n",
    "    topic_model = pickle.load(model_file)\n",
    "\n",
    "# Load the topics\n",
    "with open('topics_hate_comments.pkl', 'rb') as topics_file:\n",
    "    topics = pickle.load(topics_file)\n",
    "\n",
    "# Load the probabilities\n",
    "with open('probs_hate_comments.pkl', 'rb') as probs_file:\n",
    "    probs = pickle.load(probs_file)\n",
    "\n",
    "print(\"Topic model, topics, and probabilities have been loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a982c332-6848-451f-be4f-5e691dd7f61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_info = topic_model.get_topic_info()\n",
    "\n",
    "# This step requires having the LLM downloaded\n",
    "filename_llm = \"openhermes-2.5-mistral-7b.Q4_K_M.gguf\"\n",
    "llm = Llama(\n",
    "    model_path=filename_llm,\n",
    "    n_gpu_layers=-1,\n",
    "    n_ctx=4096,\n",
    "    stop=[\"Q:\", \"\\n\"]\n",
    ")\n",
    "\n",
    "def generate_label_for_topic(row):\n",
    "    combined_words = []\n",
    "    seen_words = set()\n",
    "\n",
    "    for source in ['KeyBERT', 'POS', 'KeyBERT_MMR']:\n",
    "        for word in row[source]:\n",
    "            if word not in seen_words:\n",
    "                seen_words.add(word)\n",
    "                combined_words.append(word)\n",
    "\n",
    "    combined_description = \" \".join(combined_words)\n",
    "\n",
    "    prompt = f\"These are the top words of a topic resulting from topic modeling: {top_words}. Provide a concise label for this topic, limited to a maximum of 5 words:\"\n",
    "    response = llm(prompt)\n",
    "\n",
    "    try:\n",
    "        print(\"Response: \", response)\n",
    "        label_text = response['choices'][0]['text'].strip()\n",
    "        return label_text\n",
    "    except (KeyError, IndexError, TypeError) as e:\n",
    "        print(\"Failed to extract label due to:\", str(e))\n",
    "        return \"Label extraction failed\"\n",
    "\n",
    "tqdm.pandas(desc=\"Generating Labels\")\n",
    "topic_info['CustomName'] = topic_info.progress_apply(generate_label_for_topic, axis=1)\n",
    "\n",
    "# Extract the topic labels for all topics\n",
    "topic_labels = topic_info.set_index('Topic')['CustomName'].to_dict()\n",
    "\n",
    "# Ensure all labels are strings and filter out \"No Label\"\n",
    "filtered_topic_labels = {k: v for k, v in topic_labels.items() if pd.notna(v) and v != \"Label extraction failed\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b9dc2e-f8e5-4dc6-a8d6-74f68847a9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_info.to_csv(\"topicInformation_hate_comments.csv\")\n",
    "topic_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda2245f-85bf-4101-ac37-15d6b4065104",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
