{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3542af1a-4d9d-4e88-a3e2-7a152a76c612",
   "metadata": {},
   "source": [
    "Assuming you have a dataframe containing video data with details such as the number of comments, the number of hate comments, the description, and the video cluster, this code calculates the results outlined in Section 6 on Hate Speech Regularities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730bb71b-c788-498d-b518-6fcc48a51aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import json\n",
    "import ast\n",
    "\n",
    "# Third-Party Library Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import load\n",
    "import shap\n",
    "\n",
    "# Scikit-Learn Imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, \n",
    "    accuracy_score\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce825e8-3b8b-4d21-b105-dd419d30a44b",
   "metadata": {},
   "source": [
    "# SHAP Values computation\n",
    "In this implementation, one can train and finetune both the logistic and linear regressions as desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bc3437-e4e1-4453-83e0-c9fdd8751326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for logistic regression\n",
    "videoData = videoDataWithOutliers[videoDataWithOutliers['bertTopicVideoFrameAndDescriptionCluster'] != -1].copy()\n",
    "videoData['has_hate_comments'] = videoData['hate_comments'] > 0\n",
    "\n",
    "# Define features and target variable\n",
    "distance_columns = [col for col in videoData.columns if 'Topic_' in col and '_distance' in col] \n",
    "X = videoData[distance_columns]\n",
    "y = videoData['has_hate_comments']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features as desired\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train the logistic regression model\n",
    "logistic_model = LogisticRegression(max_iter=1000)\n",
    "logistic_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict using logistic regression\n",
    "y_pred_logistic = logistic_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy for logistic regression\n",
    "accuracy_logistic = accuracy_score(y_test, y_pred_logistic)\n",
    "\n",
    "# SHAP values for logistic regression\n",
    "explainer_logistic = shap.LinearExplainer(logistic_model, X_train_scaled, feature_perturbation=\"correlation_dependent\")\n",
    "shap_values_logistic = explainer_logistic.shap_values(X_test_scaled)\n",
    "\n",
    "# Calculate mean absolute SHAP values for logistic regression\n",
    "shap_values_logistic_mean_abs = pd.DataFrame(shap_values_logistic, columns=distance_columns).mean()\n",
    "top_logistic_features = shap_values_logistic_mean_abs.sort_values(ascending=False).head(10)\n",
    "\n",
    "# Extract numeric identifiers from the 'Feature' column for merging\n",
    "top_logistic_features_df = top_logistic_features.reset_index().rename(columns={0: 'MeanSHAP', 'index': 'Feature'})\n",
    "top_logistic_features_df['Topic_num'] = top_logistic_features_df['Feature'].str.extract('(\\d+)').astype(int)\n",
    "\n",
    "# Merge top features with their representations\n",
    "unique_representations = videoData[['bertTopicVideoFrameAndDescriptionCluster', 'Representation']].drop_duplicates(subset=['bertTopicVideoFrameAndDescriptionCluster'])\n",
    "unique_representations['bertTopicVideoFrameAndDescriptionCluster'] = unique_representations['bertTopicVideoFrameAndDescriptionCluster'].astype(int)\n",
    "top_10_logistic = top_logistic_features_df.merge(unique_representations, left_on='Topic_num', right_on='bertTopicVideoFrameAndDescriptionCluster', how='left')\n",
    "\n",
    "# Calculate overall average SHAP values for all features\n",
    "overall_avg_shap_logistic = pd.DataFrame(shap_values_logistic, columns=distance_columns).mean().mean()\n",
    "top_10_logistic['OverallAvgSHAP'] = overall_avg_shap_logistic\n",
    "top_10_logistic['SHAP_vs_Avg'] = top_10_logistic['MeanSHAP'] / top_10_logistic['OverallAvgSHAP']\n",
    "\n",
    "# Print results for Logistic Regression\n",
    "print('Top 10 Most Influential Features from Logistic Regression (SHAP values):')\n",
    "print(top_10_logistic[['Feature', 'MeanSHAP', 'OverallAvgSHAP', 'SHAP_vs_Avg', 'Representation']])\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_logistic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e47b10-6a1d-4548-9ae7-aa57f3edb81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for linear regression\n",
    "videoData = videoDataWithOutliers[videoDataWithOutliers['bertTopicVideoFrameAndDescriptionCluster'] != -1].copy()\n",
    "videoData['hate_comment_percentage'] = videoData['hate_comments'] / videoData['total_comments']\n",
    "videoData['hate_comment_percentage'] = videoData['hate_comment_percentage'].fillna(0)  # Fill NaN values\n",
    "\n",
    "# Define features and target variable\n",
    "distance_columns = [col for col in videoData.columns if 'Topic_' in col and '_distance' in col] \n",
    "X = videoData[distance_columns]\n",
    "y_lr = videoData['hate_comment_percentage']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_lr, X_test_lr, y_train_lr, y_test_lr = train_test_split(X, y_lr, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features as desired\n",
    "scaler_lr = StandardScaler()\n",
    "X_train_lr_scaled = scaler_lr.fit_transform(X_train_lr)\n",
    "X_test_lr_scaled = scaler_lr.transform(X_test_lr)\n",
    "\n",
    "# Train the linear regression model\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train_lr_scaled, y_train_lr)\n",
    "\n",
    "# Predict with linear regression and evaluate using regression metrics\n",
    "y_pred_lr = linear_model.predict(X_test_lr_scaled)\n",
    "mse_lr = mean_squared_error(y_test_lr, y_pred_lr)\n",
    "\n",
    "# SHAP values for linear regression\n",
    "explainer_lr = shap.LinearExplainer(linear_model, X_train_lr_scaled, feature_perturbation=\"correlation_dependent\")\n",
    "shap_values_lr = explainer_lr.shap_values(X_test_lr_scaled)\n",
    "\n",
    "# Calculate mean absolute SHAP values for linear regression\n",
    "shap_values_lr_mean_abs = pd.DataFrame(shap_values_lr, columns=distance_columns).mean()\n",
    "top_lr_features = shap_values_lr_mean_abs.sort_values(ascending=False).head(10)\n",
    "\n",
    "# Extract numeric identifiers from the 'Feature' column for merging\n",
    "top_lr_features_df = top_lr_features.reset_index().rename(columns={0: 'MeanSHAP', 'index': 'Feature'})\n",
    "top_lr_features_df['Topic_num'] = top_lr_features_df['Feature'].str.extract('(\\d+)').astype(int)\n",
    "\n",
    "# Merge top features with their representations\n",
    "unique_representations = videoData[['bertTopicVideoFrameAndDescriptionCluster', 'Representation']].drop_duplicates(subset=['bertTopicVideoFrameAndDescriptionCluster'])\n",
    "unique_representations['bertTopicVideoFrameAndDescriptionCluster'] = unique_representations['bertTopicVideoFrameAndDescriptionCluster'].astype(int)\n",
    "top_10_lr = top_lr_features_df.merge(unique_representations, left_on='Topic_num', right_on='bertTopicVideoFrameAndDescriptionCluster', how='left')\n",
    "\n",
    "# Calculate overall average SHAP values for all features\n",
    "overall_avg_shap_lr = pd.DataFrame(shap_values_lr, columns=distance_columns).mean().mean()\n",
    "top_10_lr['OverallAvgSHAP'] = overall_avg_shap_lr\n",
    "top_10_lr['SHAP_vs_Avg'] = top_10_lr['MeanSHAP'] / top_10_lr['OverallAvgSHAP']\n",
    "\n",
    "# Print results for Linear Regression\n",
    "print('Top 10 Most Influential Features from Linear Regression (SHAP values):')\n",
    "print(top_10_lr[['Feature', 'MeanSHAP', 'OverallAvgSHAP', 'SHAP_vs_Avg', 'Representation']])\n",
    "print(\"Linear Regression MSE:\", mse_lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8227abc5-cab0-45bd-83bb-496bd2313137",
   "metadata": {},
   "source": [
    "# Top Clusters by hate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9141b7d-db04-4a2b-a04e-756a44dd6d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows where 'bertTopicVideoFrameAndDescriptionCluster' is -1\n",
    "videoData = videoDataWithOutliers[videoDataWithOutliers['bertTopicVideoFrameAndDescriptionCluster'] != -1]\n",
    "videoData['hate_comment_percentage'] = videoData['hate_comment_percentage'].fillna(0)\n",
    "cluster_hate_percentage = videoData.groupby('bertTopicVideoFrameAndDescriptionCluster')['hate_comment_percentage'].mean().reset_index()\n",
    "\n",
    "cluster_info = videoData[['bertTopicVideoFrameAndDescriptionCluster', 'Count', 'Representation']].drop_duplicates()\n",
    "cluster_hate_percentage = cluster_hate_percentage.merge(cluster_info, on='bertTopicVideoFrameAndDescriptionCluster', how='left')\n",
    "sorted_clusters = cluster_hate_percentage.sort_values(by='hate_comment_percentage', ascending=False)\n",
    "\n",
    "# Display the top 50 clusters ranked by hate comment percentage\n",
    "print('Top 50 Clusters ranked by hate comment percentage:')\n",
    "top_50_clusters_percentage = sorted_clusters.head(50)\n",
    "\n",
    "# Also sort the clusters by 'Count' in descending order\n",
    "sorted_clusters_by_count = cluster_hate_percentage.sort_values(by='Count', ascending=False)\n",
    "top_50_clusters_count = sorted_clusters_by_count.head(50)\n",
    "\n",
    "# Print the top clusters by percentage\n",
    "print(\"Top 50 Clusters by Hate Comment Percentage:\")\n",
    "print(top_50_clusters_percentage[['bertTopicVideoFrameAndDescriptionCluster', 'hate_comment_percentage', 'Count', 'Representation']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e91dd3-321b-4547-ad3e-7643bc5b0bf0",
   "metadata": {},
   "source": [
    "# Hashtaghs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100a31ea-3b60-4665-ae8b-e390b45dafbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract unique pairs of hashtags from a text\n",
    "def extract_hashtag_pairs(text):\n",
    "    hashtags = re.findall(r'#\\w+', text)\n",
    "    # Use itertools.combinations to find all pairs, if no pairs, just the hashtag itself\n",
    "    if len(hashtags) < 2:\n",
    "        return hashtags\n",
    "    else:\n",
    "        pairs = list(itertools.combinations(sorted(set(hashtags)), 2))\n",
    "        return pairs + hashtags  # include individual hashtags as well\n",
    "\n",
    "def process_hashtag_pairs(hashtags_df):\n",
    "    # Group by hashtag pairs and compute required metrics\n",
    "    hashtag_metrics = hashtags_df.groupby('hashtag_pairs').agg(\n",
    "        hashtag_count=('hashtag_pairs', 'size'),\n",
    "        total_comments=('total_comments', 'sum'),\n",
    "        total_hate_comments=('hate_comments', 'sum'),\n",
    "        total_hate_percentage=('hate_comment_percentage', 'sum'),\n",
    "    ).reset_index()\n",
    "\n",
    "    if 'aweme_id' in hashtags_df.columns:\n",
    "        n_videos = len(hashtags_df['aweme_id'].unique())\n",
    "    else:\n",
    "        n_videos = len(hashtags_df)\n",
    "\n",
    "    total_comments = np.sum(hashtags_df.drop_duplicates(subset=[\"aweme_id\"])[\"total_comments\"])\n",
    "\n",
    "    hashtag_metrics['average_hate_percentage'] = hashtag_metrics['total_hate_percentage'] / hashtag_metrics['hashtag_count']\n",
    "    \n",
    "    ranked_by_hate_comments = hashtag_metrics.sort_values(by='hashtag_count', ascending=False).head(400)\n",
    "    ranked_by_hate_comments = ranked_by_hate_comments.sort_values(by='total_hate_comments', ascending=False).head(50)\n",
    "    top_hate_percentage = ranked_by_hate_comments.sort_values(by='average_hate_percentage', ascending=False).head(30)\n",
    "\n",
    "    return top_hate_percentage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8eb063-99d3-47eb-a06d-b6bfd2bf5e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "videoData.loc[:, 'desc'] = videoData['desc'].fillna('')\n",
    "\n",
    "videoData.loc[:, 'hashtag_pairs'] = videoData['desc'].apply(extract_hashtag_pairs)\n",
    "hashtags_df = videoData.explode('hashtag_pairs')\n",
    "\n",
    "# Process the data\n",
    "hashtag_metrics = process_hashtag_pairs(hashtags_df)\n",
    "hashtag_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
