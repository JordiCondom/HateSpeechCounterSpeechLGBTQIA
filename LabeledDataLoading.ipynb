{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming you have all the datasets referenced in the thesis both for HS and CS downloaded, this code both cleans the datasets and brings them together. In the case of CS, it groups them to make pairs of subsequent comments/messages in the dialogue. The output of this code are the files LabeledHateTrainingDataset.csv and LabeledDialoguesForCounterTraining.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-29 18:21:20.218089: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-29 18:21:20.246929: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-29 18:21:20.779031: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Standard Library Imports\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Third-Party Library Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# Hugging Face Transformers and Datasets\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, \n",
    "    AutoTokenizer, \n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    DataCollatorWithPadding, \n",
    "    EarlyStoppingCallback, \n",
    "    pipeline\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bring Labeled Datasets of Hate Speech Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Datasets from each source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ethos Dataset\n",
    "https://github.com/intelligence-csd-auth-gr/Ethos-Hate-Speech-Dataset/tree/master\n",
    "\n",
    "Provides a total of 94 hate comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ethos_file_path = '../../DATA_Data/Ethos_Dataset_Multi_Label.csv'\n",
    "ethos_data = pd.read_csv(ethos_file_path,sep=';')\n",
    "ethos_filtered_data = ethos_data[ethos_data['sexual_orientation'] > 0]\n",
    "ethos_dataset = ethos_filtered_data[['comment']]\n",
    "ethos_dataset = ethos_dataset.rename(columns={'comment': 'text'})\n",
    "ethos_dataset['label'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FRENKT-en-hate-datasetInspection\n",
    "The test dataset has a total of 1017 rows.\n",
    "The train dataset has a total of 4337 rows.\n",
    "The dev dataset has a total of 482 rows.\n",
    "That makes a total of 5836\n",
    "! Here we are only taking the 4337 rows of training\n",
    "\n",
    "https://huggingface.co/datasets/classla/FRENK-hate-en?row=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRENKT_train_path = '../../DATA_Data/train.tsv'\n",
    "\n",
    "columns = ['ID', 'Comment', 'Background', 'Offensive', 'Target', 'Category']\n",
    "FRENK_train_dataset = pd.read_csv(FRENKT_train_path, sep='\\t', header=None, names=columns)\n",
    "\n",
    "FRENK_train_lgbt = FRENK_train_dataset[FRENK_train_dataset['Category'] == 'lgbt'].copy()\n",
    "FRENK_train_lgbt.loc[:, 'Label'] = (FRENK_train_lgbt['Offensive'] == 'Offensive').astype(int)\n",
    "FRENK_train_lgbt = FRENK_train_lgbt[['Comment', 'Label']]\n",
    "FRENK_train_lgbt = FRENK_train_lgbt.rename(columns={\n",
    "    'Comment': 'text',\n",
    "    'Label': 'label'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset-for-Identification-of-Queerphobia\n",
    "It has a total of 10000 rows: https://github.com/ShivumB/Dataset-for-Identification-of-Queerphobia/tree/main \n",
    "\n",
    "Paper: https://www.researchgate.net/publication/370504802_Dataset_for_identification_of_queerphobia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "queer_phobia_file_path = '../../DATA_Data/queerPhobia.csv'\n",
    "\n",
    "queer_phobia_dataset= pd.read_csv(queer_phobia_file_path)\n",
    "queer_phobia_dataset = queer_phobia_dataset.rename(columns={\n",
    "    'classification':'label'\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hatecheck-data\n",
    "\n",
    "It has a total of 1014 hate messages directed towards gay people or trans people\n",
    "\n",
    "https://github.com/paul-rottger/hatecheck-data/tree/main\n",
    "\n",
    "Paper:https://aclanthology.org/2021.acl-long.4.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hatecheck_file_path = '../../DATA_Data/test_suite_cases.csv'\n",
    "\n",
    "hatecheck_data = pd.read_csv(hatecheck_file_path)\n",
    "hatecheck_data_lgbtq = hatecheck_data[hatecheck_data['target_ident'].isin(['gay people', 'trans people'])]\n",
    "\n",
    "hatecheck_dataset = hatecheck_data_lgbtq[['test_case', 'label_gold']].copy()\n",
    "hatecheck_dataset.rename(columns={'test_case': 'text', 'label_gold': 'label'}, inplace=True)\n",
    "hatecheck_dataset['label'] = (hatecheck_dataset['label'] == 'hateful').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HateXpain data\n",
    "\n",
    "A total of 1940 sentences: https://github.com/hate-alert/HateXplain/tree/master/Data\n",
    "\n",
    "Paper: HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../DATA_Data/dataset.json', 'r') as file:\n",
    "    HateXplain_data = json.load(file)\n",
    "\n",
    "for post_id, post_data in HateXplain_data.items():\n",
    "    post_data['post_tokens'] = ' '.join(post_data['post_tokens'])\n",
    "\n",
    "unique_targets = set()\n",
    "\n",
    "for post_id, post_data in HateXplain_data.items():\n",
    "    for annotator in post_data['annotators']:\n",
    "        unique_targets.update(annotator['target'])\n",
    "\n",
    "HateXplain_filtered_data = {}\n",
    "\n",
    "for post_id, post_data in HateXplain_data.items():\n",
    "    count = sum(1 for annotator in post_data['annotators'] if any(target in annotator['target'] for target in ['Homosexual', 'Asexual', 'Bisexual']))\n",
    "    if count >= 2:\n",
    "        HateXplain_filtered_data[post_id] = post_data\n",
    "\n",
    "for post_id, post_data in HateXplain_filtered_data.items():\n",
    "    post_data.pop('post_id', None)\n",
    "    post_data.pop('rationales', None)\n",
    "    target_vectors = []\n",
    "    label_vector = []\n",
    "    for annotator in post_data['annotators']:\n",
    "        target_vectors.extend(annotator['target'])\n",
    "        label_vector.append(annotator['label'])\n",
    "    post_data[\"label\"] = label_vector\n",
    "    post_data['targets'] = target_vectors\n",
    "    post_data.pop('annotators', None)\n",
    "\n",
    "data_list = [{'text': data['post_tokens'], 'label': data['label']} for key, data in HateXplain_filtered_data.items()]\n",
    "\n",
    "HateXplain_dataset = pd.DataFrame(data_list)\n",
    "\n",
    "def sort_and_join(label):\n",
    "    return ', '.join(sorted(label))\n",
    "\n",
    "unique_sorted_labels = HateXplain_dataset['label'].apply(sort_and_join).unique()\n",
    "\n",
    "def most_common_label(label):\n",
    "    if not label:\n",
    "        return \"hatespeech\"\n",
    "    counts = Counter(label)\n",
    "    return counts.most_common(1)[0][0]\n",
    "\n",
    "HateXplain_dataset['label'] = HateXplain_dataset['label'].apply(most_common_label)\n",
    "\n",
    "def map_label(label):\n",
    "    return 0 if label == \"normal\" else 1\n",
    "\n",
    "HateXplain_dataset['label'] = HateXplain_dataset['label'].apply(map_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SBIC\n",
    "\n",
    "2770 rows\n",
    "\n",
    "https://maartensap.com/social-bias-frames/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SBIC_train = pd.read_csv(\"../../DATA_Data/SBIC.v2/SBIC.v2.trn.csv\")\n",
    "SBIC_dev = pd.read_csv(\"../../DATA_Data/SBIC.v2/SBIC.v2.dev.csv\")\n",
    "SBIC_tst = pd.read_csv(\"../../DATA_Data/SBIC.v2/SBIC.v2.tst.csv\")\n",
    "\n",
    "SBIC_train = pd.concat([SBIC_train, SBIC_dev, SBIC_tst], ignore_index=True)\n",
    "\n",
    "columns_to_remove = [\"annotatorGender\", \"annotatorMinority\", \"speakerMinorityYN\",\"WorkerId\", \"HITId\",\n",
    "                     \"annotatorPolitics\", \"annotatorRace\", \"annotatorAge\", \"dataSource\", \"sexReason\",\n",
    "                     \"whoTarget\", \"intentYN\", \"sexYN\", \"targetCategory\"]\n",
    "\n",
    "SBIC_train = SBIC_train.drop(columns_to_remove, axis=1)# Print the count of values in the 'exampleColumn'\n",
    "groups_to_include = [\"gay men\", \"lesbian women\", \"lesbian women, gay men\", \"women, gay men\",\n",
    "                         \"gay men, trans women, trans men, bisexual women, bisexual men\",\n",
    "                         \"women, lesbian women, trans women, bisexual women\",\n",
    "                         \"lesbian women, gay men, trans women, trans men, bisexual women, bisexual men\",\n",
    "                         \"trans women, trans men\", \"gays\", \"trans men\", \"Non - binary\",\n",
    "                         \"women, trans men\", \"trans women\", \"bisexual women, bisexual men\", \"women, trans women\",\n",
    "                         \"women, gay men\", \"women, lesbian women\", \"lesbian women, gay men, bisexual women, bisexual men\",\n",
    "                         \"bisexual women\", \"trans people\", \"gay people\", \"asexual people\"]\n",
    "\n",
    "keywords = [\"gay\", \"lesbian\", \"trans\", \"transgender\", \"bisexual\", \"asexual\", \"Non-binary\", \"Non - binary\", \"queer\",\n",
    "            \"Queer\", \"Demi-queer\", \"Fluid\", \"fluid\", \"Pan Sexual\"]\n",
    "\n",
    "SBIC_filtered_df = SBIC_train[SBIC_train['targetMinority'].isin(groups_to_include)]\n",
    "\n",
    "for keyword in keywords:\n",
    "    SBIC_filtered_df = pd.concat([\n",
    "        SBIC_filtered_df, \n",
    "        SBIC_train[SBIC_train['targetMinority'].str.contains(keyword, case=False, na=False)]\n",
    "    ], ignore_index=True)\n",
    "\n",
    "SBIC_filtered_df = SBIC_filtered_df.drop_duplicates()\n",
    "SBIC_filtered_df = SBIC_filtered_df[SBIC_filtered_df['offensiveYN'] == 1]\n",
    "SBIC_dataset = pd.DataFrame({\n",
    "    'text': pd.concat([SBIC_filtered_df['post'], SBIC_filtered_df['targetStereotype']]).dropna().unique()\n",
    "})\n",
    "\n",
    "SBIC_dataset = SBIC_dataset.drop_duplicates()\n",
    "SBIC_dataset[\"label\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONAN\n",
    "\n",
    "465 rows \n",
    "\n",
    "https://github.com/marcoguerini/CONAN/tree/master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "conan_DATA = pd.read_csv(\"../../DATA_Data/Multitarget-CONAN.csv\")\n",
    "\n",
    "\n",
    "conan_DATA_LGBTQ = conan_DATA[conan_DATA[\"TARGET\"] == \"LGBT+\"]\n",
    "conan_DATA_LGBTQ = conan_DATA_LGBTQ[[\"HATE_SPEECH\"]]\n",
    "conan_DATA_LGBTQ[\"label\"] = 1\n",
    "conan_DATA_LGBTQ = conan_DATA_LGBTQ.rename(columns={'HATE_SPEECH': 'text'})\n",
    "conan_DATA_LGBTQ = conan_DATA_LGBTQ.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ucberkeley-dlab\n",
    "\n",
    "https://huggingface.co/datasets/ucberkeley-dlab/measuring-hate-speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "berkeley_dataset = datasets.load_dataset('ucberkeley-dlab/measuring-hate-speech', 'default')   \n",
    "berkeley_dataframe = berkeley_dataset['train'].to_pandas()\n",
    "\n",
    "\n",
    "columns_to_keep = [\n",
    "    \"hate_speech_score\",\n",
    "    \"text\",\n",
    "    \"target_gender_non_binary\",\n",
    "    \"target_gender_transgender_men\",\n",
    "    \"target_gender_transgender_unspecified\",\n",
    "    \"target_gender_transgender_women\",\n",
    "    \"target_gender_other\",\n",
    "    \"target_sexuality_bisexual\",\n",
    "    \"target_sexuality_gay\",\n",
    "    \"target_sexuality_lesbian\",\n",
    "    \"target_sexuality_other\",\n",
    "    \"target_sexuality\",\n",
    "]\n",
    "\n",
    "berkeley_dataframe = berkeley_dataframe[columns_to_keep]\n",
    "columns_to_check = [\n",
    "    'target_gender_non_binary', 'target_gender_transgender_men',\n",
    "    'target_gender_transgender_unspecified', 'target_gender_transgender_women',\n",
    "    'target_gender_other', 'target_sexuality_bisexual', 'target_sexuality_gay',\n",
    "    'target_sexuality_lesbian', 'target_sexuality_other', \"target_sexuality\"\n",
    "]\n",
    "\n",
    "# Create a condition that checks if any of the specified columns have True\n",
    "condition = berkeley_dataframe[columns_to_check].any(axis=1)\n",
    "\n",
    "# Filter the DataFrame based on the condition\n",
    "filtered_berkeley_dataframe = berkeley_dataframe[condition]\n",
    "\n",
    "berkeley_hate_speech_dataset = filtered_berkeley_dataframe[[\"hate_speech_score\", \"text\"]]\n",
    "berkeley_counter_speech_dataset = filtered_berkeley_dataframe[[\"hate_speech_score\", \"text\"]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_57538/3613923650.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  berkeley_hate_speech_dataset['hate_speech_score'] = berkeley_hate_speech_dataset['hate_speech_score'].apply(lambda x: 1 if x > 0.5 else 0)\n"
     ]
    }
   ],
   "source": [
    "berkeley_hate_speech_dataset['hate_speech_score'] = berkeley_hate_speech_dataset['hate_speech_score'].apply(lambda x: 1 if x > 0.5 else 0)\n",
    "berkeley_hate_speech_dataset = berkeley_hate_speech_dataset.rename(columns={'hate_speech_score': 'label'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join all labeled hate speech datasets\n",
    "30471 texts:\n",
    "- 19119 non hate speech\n",
    "- 11352 hate speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>why haven't you faggots taken the bi-pill yet</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why does Bulgaria exist as a country? It's one...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what happened in the good old days when these ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>transgenders need help...they are sick in the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trans are using the same logic feminists have ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44614</th>\n",
       "      <td>Lailat al Miraj mubarak to all Muslims.. I'm w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44840</th>\n",
       "      <td>Notwithstanding Marriyum Aurangzeb sahiba's po...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44949</th>\n",
       "      <td>I, a Catholic and a Jesuit, am grateful for th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45777</th>\n",
       "      <td>NUGS lauds Ghanaian Muslims for their immense ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48422</th>\n",
       "      <td>#DSSUpliftsTheThirdGender #EmancipationOfEunuc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30471 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0          why haven't you faggots taken the bi-pill yet      1\n",
       "1      Why does Bulgaria exist as a country? It's one...      1\n",
       "2      what happened in the good old days when these ...      1\n",
       "3      transgenders need help...they are sick in the ...      1\n",
       "4      Trans are using the same logic feminists have ...      1\n",
       "...                                                  ...    ...\n",
       "44614  Lailat al Miraj mubarak to all Muslims.. I'm w...      0\n",
       "44840  Notwithstanding Marriyum Aurangzeb sahiba's po...      0\n",
       "44949  I, a Catholic and a Jesuit, am grateful for th...      0\n",
       "45777  NUGS lauds Ghanaian Muslims for their immense ...      0\n",
       "48422  #DSSUpliftsTheThirdGender #EmancipationOfEunuc...      0\n",
       "\n",
       "[30471 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_hate_df = pd.concat([ethos_dataset, FRENK_train_lgbt, queer_phobia_dataset, \n",
    "                         hatecheck_dataset, HateXplain_dataset, SBIC_dataset,\n",
    "                         conan_DATA_LGBTQ,berkeley_hate_speech_dataset], ignore_index=True)\n",
    "final_hate_df = final_hate_df.drop_duplicates(subset=[\"text\"])\n",
    "final_hate_df.to_csv('LabeledHateTrainingDataset.csv', index=False)\n",
    "final_hate_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bring Labeled Datasets of Counter Speech Together\n",
    "\n",
    "4213 conversations, each with two comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialoconan = pd.read_csv(\"../../CODES_context/Data/DIALOCONAN.csv\")\n",
    "dialoconan =  dialoconan[[\"text\", \"TARGET\", \"dialogue_id\", \"turn_id\", \"type\"]]\n",
    "dialoconan = dialoconan[dialoconan[\"TARGET\"].isin([\"LGBT+\", \"WOMEN/LGBT+\"])]\n",
    "\n",
    "\n",
    "multitarget_conan = pd.read_csv(\"../../CODES_context/Data/Multitarget-CONAN.csv\")\n",
    "multitarget_conan = multitarget_conan[[\"INDEX\", \"HATE_SPEECH\", \"COUNTER_NARRATIVE\", \"TARGET\"]]\n",
    "multitarget_conan = multitarget_conan.rename(columns={\n",
    "    'INDEX': 'dialogue_id',\n",
    "    'HATE_SPEECH': 'hateSpeech',\n",
    "    'COUNTER_NARRATIVE': 'counterSpeech',\n",
    "    'TARGET': 'TARGET'\n",
    "})\n",
    "\n",
    "rows = []\n",
    "\n",
    "for index, row in multitarget_conan.iterrows():\n",
    "    # Convert dialogue_id to an integer and add 3049\n",
    "    new_dialogue_id = str(int(row['dialogue_id']) + 3049)\n",
    "\n",
    "    # Create a row for hateSpeech\n",
    "    hs_row = {\n",
    "        'dialogue_id': new_dialogue_id,\n",
    "        'TARGET': row['TARGET'],\n",
    "        'text': row['hateSpeech'],\n",
    "        'turn_id': 0,\n",
    "        'type': 'HS'\n",
    "    }\n",
    "    rows.append(hs_row)\n",
    "\n",
    "    # Create a row for counterSpeech\n",
    "    cn_row = {\n",
    "        'dialogue_id': new_dialogue_id,\n",
    "        'TARGET': row['TARGET'],\n",
    "        'text': row['counterSpeech'],\n",
    "        'turn_id': 1,\n",
    "        'type': 'CN'\n",
    "    }\n",
    "    rows.append(cn_row)\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "transformed_multitarget_conan = pd.DataFrame(rows)\n",
    "transformed_multitarget_conan = transformed_multitarget_conan[transformed_multitarget_conan[\"TARGET\"].isin([\"LGBT+\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicated text set 1:\n",
      " - dialogue_id: 29\n",
      "   * LGBT history month now being taught in schools… I’m sick of this being shoved down our throats, what happened to British, European and World history being taught? Identity politics gone mad.\n",
      "   * We still can teach British, European and World history, but part of that is looking at the history of groups which have been erased from our history previously. Looking at the some of the historic struggles of LGBT people in the UK can inform the future for everyone!\n",
      "   * Do we have to discuss the sexuality of everyone in history when it’s largely irrelevant? If they were persecuted for it then fair enough, stick it in there and say how wrong it was, otherwise leave well alone.\n",
      "   * LGBT people have been persecuted until recently in the UK, and it's still illegal to be gay in over 70 countries. I think it's important that people see that there have always been LGBT people, throughout history and that LGBT identities are not 'new' or a 'trend', they're real.\n",
      " - dialogue_id: 79\n",
      "   * LGBT history month now being taught in schools… I’m sick of this being shoved down our throats, what happened to British, European and World history being taught? Identity politics gone mad.\n",
      "   * We still can teach British, European and World history, but part of that is looking at the history of groups which have been erased from our history previously. Looking at the some of the historic struggles of LGBT people in the UK can inform the future for everyone!\n",
      "   * Do we have to discuss the sexuality of everyone in history when it’s largely irrelevant? If they were persecuted for it then fair enough, stick it in there and say how wrong it was, otherwise leave well alone.\n",
      "   * LGBT people have been persecuted until recently in the UK, and it's still illegal to be gay in over 70 countries. I think it's important that people see that there have always been LGBT people, throughout history and that LGBT identities are not 'new' or a 'trend', they're real.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined_dataframe = pd.concat([transformed_multitarget_conan, dialoconan], ignore_index=True)\n",
    "combined_dataframe['dialogue_id'] = combined_dataframe['dialogue_id'].astype(str)\n",
    "\n",
    "grouped = combined_dataframe.groupby('dialogue_id')['text'].apply(lambda x: tuple(x)).reset_index()\n",
    "\n",
    "# Find duplicate text sets\n",
    "duplicates = grouped[grouped.duplicated(subset=['text'], keep=False)]\n",
    "# Group by the text tuples and find dialogue_id pairs\n",
    "duplicate_pairs = duplicates.groupby('text')['dialogue_id'].apply(list).reset_index()\n",
    "\n",
    "# Print the pairs of dialogues that are duplicated along with their texts\n",
    "for index, row in duplicate_pairs.iterrows():\n",
    "    dialogue_ids = row['dialogue_id']\n",
    "    texts = row['text']\n",
    "    if len(dialogue_ids) > 1:\n",
    "        print(f\"Duplicated text set {index + 1}:\")\n",
    "        for dialogue_id in dialogue_ids:\n",
    "            print(f\" - dialogue_id: {dialogue_id}\")\n",
    "            # Print the texts within each dialogue\n",
    "            dialogue_texts = combined_dataframe[combined_dataframe['dialogue_id'] == dialogue_id]['text'].tolist()\n",
    "            for text in dialogue_texts:\n",
    "                print(f\"   * {text}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "# Identify all dialogue_ids to be removed (keep only the first occurrence)\n",
    "to_remove = []\n",
    "for index, row in duplicate_pairs.iterrows():\n",
    "    dialogue_ids = row['dialogue_id']\n",
    "    if len(dialogue_ids) > 1:\n",
    "        to_remove.extend(dialogue_ids[:-1])  # Keep the first occurrence, mark the rest for removal\n",
    "\n",
    "# Remove the identified duplicates from the original dataframe\n",
    "cleaned_dataframe = combined_dataframe[~combined_dataframe['dialogue_id'].isin(to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dialogue_id</th>\n",
       "      <th>text</th>\n",
       "      <th>turn_id</th>\n",
       "      <th>type</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3073_1</td>\n",
       "      <td>UK Education Secretary: ‘All children from the...</td>\n",
       "      <td>0</td>\n",
       "      <td>HS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3073_1</td>\n",
       "      <td>We are always developing as time goes on and s...</td>\n",
       "      <td>1</td>\n",
       "      <td>CN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3076_1</td>\n",
       "      <td>Since when has abusing the anus as a sex organ...</td>\n",
       "      <td>0</td>\n",
       "      <td>HS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3076_1</td>\n",
       "      <td>To recognise and accept practices that we don'...</td>\n",
       "      <td>1</td>\n",
       "      <td>CN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3079_1</td>\n",
       "      <td>UK Education Secretary: ‘All children from the...</td>\n",
       "      <td>0</td>\n",
       "      <td>HS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dialogue_id                                               text  turn_id  \\\n",
       "0      3073_1  UK Education Secretary: ‘All children from the...        0   \n",
       "1      3073_1  We are always developing as time goes on and s...        1   \n",
       "2      3076_1  Since when has abusing the anus as a sex organ...        0   \n",
       "3      3076_1  To recognise and accept practices that we don'...        1   \n",
       "4      3079_1  UK Education Secretary: ‘All children from the...        0   \n",
       "\n",
       "  type  label  \n",
       "0   HS      0  \n",
       "1   CN      1  \n",
       "2   HS      0  \n",
       "3   CN      1  \n",
       "4   HS      0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify the pairs of dialogues we are interested in and separate and label them accordingly \n",
    "subdialogue_combinations = [(0, 1), (2, 3), (4, 5), (6,7), (1,2), (3,4), (5,6), (0,2), (2,4), (4,6)]\n",
    "subdialogues = []\n",
    "\n",
    "for dialogue_id in cleaned_dataframe['dialogue_id'].unique():\n",
    "    subdialogue_counter = 1\n",
    "    dialogue_df = cleaned_dataframe[cleaned_dataframe['dialogue_id'] == dialogue_id]\n",
    "\n",
    "    for combo in subdialogue_combinations:\n",
    "        if all(turn_id in dialogue_df['turn_id'].values for turn_id in combo):\n",
    "            subdialogue = dialogue_df[dialogue_df['turn_id'].isin(combo)].copy()\n",
    "            new_dialogue_id = f\"{dialogue_id}_{subdialogue_counter}\"\n",
    "            subdialogue['dialogue_id'] = new_dialogue_id\n",
    "            subdialogue_counter += 1\n",
    "            subdialogues.append(subdialogue)\n",
    "\n",
    "subdialogues_df = pd.concat(subdialogues).reset_index(drop=True)\n",
    "subdialogues_df = subdialogues_df.drop(columns=['TARGET'])\n",
    "\n",
    "subdialogues_df['label'] = subdialogues_df['type'].apply(lambda x: 1 if x == 'CN' else 0)\n",
    "subdialogues_df.to_csv(\"LabeledDialoguesForCounterTraining.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
