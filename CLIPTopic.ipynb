{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2655fb7a-b50c-44d3-8d83-17761fbe6a77",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# !Note:\n",
    "This is a preliminary implementation/idea that has to be further tested and validated. It may have bugs or stupid mistakes, but shows the general idea of using video embeddings in BERT Topic rather than text embeddings.\n",
    "\n",
    "\n",
    "# General Explanation\n",
    "This code shows how to implement BERTopic for video topic modeling and clustering. BERTopic is a technique used to extract topics from a set of documents through several steps:\n",
    "\n",
    "- Embedding Documents: Typically, BERT or similar models are used to create embeddings for text documents. However, this approach can be adapted to use any type of embeddings. In this case, X-CLIP is used, an \"extension\" of CLIP, to generate embeddings from video frames.\n",
    "- Dimensionality Reduction: The high-dimensional embeddings are reduced to a lower-dimensional space using UMAP.\n",
    "- Clustering: The reduced embeddings are clustered using HDBSCAN.\n",
    "- Topic Representation: Topics are represented by key terms extracted from the clusters. This is done using c-TF-IDF (class-based Term Frequency-Inverse Document Frequency) to generate candidate terms and MMR (Maximal Marginal Relevance) to select the most relevant terms for the best possible topic representation.\n",
    "\n",
    "In traditional BERTopic applications, text documents are embedded and these same texts are used for topic representation. In this implementation, we use video frame embeddings obtained via X-CLIP, but use text descriptions of the videos (such as the description or transcriptions) to represent the topics.\n",
    "\n",
    "\n",
    "# References:\n",
    "- X-CLIP:\n",
    "    - HuggingFace: https://huggingface.co/docs/transformers/model_doc/xclip\n",
    "    - Paper: X-CLIP: End-to-End Multi-grained Contrastive Learning for Video-Text Retrieval (https://arxiv.org/pdf/2207.0728)\n",
    "- BERT Topic:\n",
    "    - https://medium.com/data-reply-it-datatech/bertopic-topic-modeling-as-you-have-never-seen-it-before-abb48bbab2b2\n",
    "    - https://maartengr.github.io/BERTopic/index.html#visualizations\n",
    "    - Paper: BERTopic: Neural topic modeling with a class-based TF-IDF procedure (https://arxiv.org/abs/2203.05794)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e84907-6cbe-4f3e-ba58-75027141823c",
   "metadata": {},
   "source": [
    "# Imports and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9cc85c-313a-459e-87da-add63d044267",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-19 11:22:44.495225: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-19 11:22:44.553365: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-19 11:22:45.153814: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import av\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "import json\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d1b836-1c09-443c-850f-951abf5868d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract video features\n",
    "def get_video_features(video_path, clip_len=8, frame_sample_rate=1):\n",
    "    container = av.open(video_path)\n",
    "    indices = sample_frame_indices(clip_len, frame_sample_rate, container.streams.video[0].frames)\n",
    "    video = read_video_pyav(container, indices)\n",
    "\n",
    "    inputs = processor(videos=list(video), return_tensors=\"pt\")\n",
    "    video_features = model.get_video_features(**inputs)\n",
    "\n",
    "    return video_features\n",
    "\n",
    "# Function to decode video with PyAV decoder\n",
    "def read_video_pyav(container, indices):\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            frames.append(frame)\n",
    "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "\n",
    "# Function to sample frame indices from the video\n",
    "def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
    "    converted_len = int(clip_len * frame_sample_rate)\n",
    "    end_idx = np.random.randint(converted_len, seg_len)\n",
    "    start_idx = end_idx - converted_len\n",
    "    indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
    "    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
    "    return indices\n",
    "\n",
    "# Function to save progress to a JSON file\n",
    "def save_progress(json_path, video_features_list, errors):\n",
    "    data_to_save = {\n",
    "        'video_features': video_features_list,\n",
    "        'errors': errors\n",
    "    }\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(data_to_save, f)\n",
    "\n",
    "# Function to load progress from a JSON file\n",
    "def load_progress(json_path):\n",
    "    if os.path.exists(json_path):\n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return data['video_features'], data['errors']\n",
    "    else:\n",
    "        return [], {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d0156f-668a-48d5-a68e-4f5f5c3f9109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract video features\n",
    "def get_video_features(video_path, clip_len=8, frame_sample_rate=1):\n",
    "    container = av.open(video_path)\n",
    "    indices = sample_frame_indices(clip_len, frame_sample_rate, container.streams.video[0].frames)\n",
    "    video = read_video_pyav(container, indices)\n",
    "\n",
    "    inputs = processor(videos=list(video), return_tensors=\"pt\")\n",
    "    video_features = model.get_video_features(**inputs)\n",
    "\n",
    "    return video_features\n",
    "\n",
    "# Function to decode video with PyAV decoder\n",
    "def read_video_pyav(container, indices):\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            frames.append(frame)\n",
    "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "\n",
    "# Function to sample frame indices from the video\n",
    "def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
    "    converted_len = int(clip_len * frame_sample_rate)\n",
    "    end_idx = np.random.randint(converted_len, seg_len)\n",
    "    start_idx = end_idx - converted_len\n",
    "    indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
    "    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
    "    return indices\n",
    "\n",
    "# Function to save progress to a JSON file\n",
    "def save_progress(json_path, video_features_list, errors):\n",
    "    data_to_save = {\n",
    "        'video_features': video_features_list,\n",
    "        'errors': errors\n",
    "    }\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(data_to_save, f)\n",
    "\n",
    "# Function to load progress from a JSON file\n",
    "def load_progress(json_path):\n",
    "    if os.path.exists(json_path):\n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return data['video_features'], data['errors']\n",
    "    else:\n",
    "        return [], {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f308e4c7-fc27-4d22-bbc1-f2bf03360641",
   "metadata": {},
   "source": [
    "# Obtain Video Features/Embeddings\n",
    "\n",
    "Input: video paths\n",
    "Output: video embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ea46ce-f974-4eab-b260-71c566aa2ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"microsoft/xclip-base-patch32\")\n",
    "model = AutoModel.from_pretrained(\"microsoft/xclip-base-patch32\")\n",
    "\n",
    "# Mock data for video paths\n",
    "mock_video_paths = ['video1.mp4', 'video2.mp4', 'video3.mp4']\n",
    "mock_data = {'video_path': mock_video_paths}\n",
    "\n",
    "# Save mock data to a JSON file\n",
    "with open('mock_data.json', 'w') as f:\n",
    "    json.dump(mock_data, f)\n",
    "\n",
    "# Load mock data\n",
    "with open('mock_data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "json_path = 'video_features.json'\n",
    "video_features_list, errors = load_progress(json_path)\n",
    "start_index = len(video_features_list)\n",
    "\n",
    "for i, video_path in enumerate(tqdm(data['video_path'][start_index:], desc=\"Processing videos\", initial=start_index)):\n",
    "    try:\n",
    "        video_features = get_video_features(video_path)\n",
    "        video_features_list.append({\n",
    "            'video_path': video_path,\n",
    "            'features': video_features.detach().cpu().numpy().tolist()\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing video {video_path}: {e}\")\n",
    "        errors[video_path] = str(e)\n",
    "        video_features_list.append({\n",
    "            'video_path': video_path,\n",
    "            'features': None\n",
    "        })\n",
    "    \n",
    "    # Save progress every 100 videos\n",
    "    if (i + 1) % 100 == 0:\n",
    "        save_progress(json_path, video_features_list, errors)\n",
    "\n",
    "# Save any remaining progress\n",
    "save_progress(json_path, video_features_list, errors)\n",
    "\n",
    "# Convert list of features to a single numpy array\n",
    "video_features_array = np.vstack([np.array(item['features']) for item in video_features_list if item['features'] is not None])\n",
    "\n",
    "# Convert numpy array to a tensor\n",
    "video_features = torch.tensor(video_features_array)\n",
    "\n",
    "# Save the video features tensor\n",
    "torch.save(video_features, 'video_features.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8251990-6cfb-4dea-bb05-91147a7b5643",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Load Video Features and Match Descriptions\n",
    "Given the video embeddings and before BERT Topic, match them with their descriptions/text that will be used for the c-TF-IDF and MMR for topic representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99f96ae-3d15-4cb5-88df-d6b5868d02a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock data for video descriptions\n",
    "mock_videoData = pd.DataFrame({\n",
    "    'video_path': ['video1.mp4', 'video2.mp4', 'video3.mp4'],\n",
    "    'desc': ['A video about cats', 'A video about dogs', 'A video about birds']\n",
    "})\n",
    "\n",
    "# Save mock data to a CSV file\n",
    "mock_videoData.to_csv('mock_videoData.csv', index=False)\n",
    "\n",
    "# Load video features\n",
    "json_path = 'video_features.json'\n",
    "with open(json_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "video_features_list = data['video_features']\n",
    "len(video_features_list)\n",
    "\n",
    "# Load video descriptions\n",
    "videoData = pd.read_csv('mock_videoData.csv')\n",
    "\n",
    "valid_descriptions = []\n",
    "valid_features = []\n",
    "\n",
    "for item in video_features_list:\n",
    "    if item['features'] is not None:\n",
    "        video_path = item['video_path']\n",
    "        features = np.array(item['features'])  # Convert to numpy array\n",
    "        \n",
    "        # Reduce features to 2D by averaging over the sequence dimension if necessary\n",
    "        # This will depend on the scenario\n",
    "        if features.shape[0] == 1:\n",
    "            features = features.squeeze(axis=0)\n",
    "            print(f\"Squeezed dimensions of features for video {video_path}: {features.shape}\")\n",
    "        \n",
    "        # Match video path to description in videoData DataFrame\n",
    "        description = videoData.loc[videoData['video_path'] == video_path, 'desc'].values\n",
    "        if description.size > 0:\n",
    "            valid_descriptions.append(str(description[0]))\n",
    "            valid_features.append(features)\n",
    "\n",
    "# Convert valid_features to a numpy array\n",
    "valid_features_array = np.array(valid_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6eab03-371f-4fe6-a8a5-92561aaae790",
   "metadata": {},
   "source": [
    "# BERT Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5d7e34-e824-4a7f-99ad-36b69a727dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess descriptions\n",
    "def preprocess_descriptions(descriptions):\n",
    "    # Add any desired preprocessing\n",
    "    processed_descriptions = [description.lower() for description in descriptions]  # Example preprocessing step\n",
    "    return processed_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce651fbc-4174-4413-a2f6-2ccc9038754f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BERTopic model\n",
    "vectorizer_model = CountVectorizer(stop_words='english')\n",
    "topic_model = BERTopic(vectorizer_model=vectorizer_model)\n",
    "\n",
    "\n",
    "\n",
    "processed_descriptions = preprocess_descriptions(valid_descriptions)\n",
    "\n",
    "# Fit the BERTopic model with the valid descriptions and embeddings\n",
    "topics, probabilities = topic_model.fit_transform(processed_descriptions, embeddings=valid_features_array)\n",
    "\n",
    "# Add topics to the DataFrame\n",
    "videoData['topic'] = None\n",
    "valid_paths = [item['video_path'] for item in video_features_list if item['features'] is not None]\n",
    "for idx, video_path in enumerate(valid_paths):\n",
    "    videoData.loc[videoData['video_path'] == video_path, 'topic'] = topics[idx]\n",
    "\n",
    "# Display topic information\n",
    "topic_info = topic_model.get_topic_info()\n",
    "print(topic_info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a40bec-92f6-46a3-b3c7-c9a07001ac6b",
   "metadata": {},
   "source": [
    "# Create html for visualization:\n",
    "Given the videos, clusters and topics representations: generates an html with 10 examples of videos of a cluster. The videos are represented by (only) their first frame. When creating the html, important to check that the video/frame paths are the adequate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfb3206-6b76-49cb-85cb-ae5e21cc7013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Function to extract the first frame from a video\n",
    "def extract_first_frame(video_path, output_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    success, frame = cap.read()\n",
    "    if success:\n",
    "        cv2.imwrite(output_path, frame)\n",
    "    cap.release()\n",
    "\n",
    "# Function to create an HTML file for visualizing the topics\n",
    "def create_html_for_topics(video_data, topic_model, output_dir):\n",
    "    topic_info = topic_model.get_topic_info()\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    html_content = \"\"\"<!DOCTYPE html>\n",
    "    <html>\n",
    "    <head>\n",
    "        <title>Video Topics</title>\n",
    "        <style>\n",
    "            body { font-family: Arial, sans-serif; }\n",
    "            h2 { margin-top: 40px; }\n",
    "            .topic-container { margin-bottom: 30px; }\n",
    "            .video-container { display: flex; flex-wrap: wrap; }\n",
    "            .video-item { margin: 10px; text-align: center; }\n",
    "            .video-item img { width: 320px; height: auto; }\n",
    "            .topic-words { font-size: 20px; font-weight: bold; }\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\"\"\"\n",
    "\n",
    "    for topic_num in topic_info['Topic']:\n",
    "        if topic_num == -1:\n",
    "            continue  # Skip the outlier topic\n",
    "        topic_videos = video_data[video_data['topic'] == topic_num].head(10)  # Limit to 10 examples\n",
    "        topic_words = topic_model.get_topic(topic_num)\n",
    "        html_content += \"<div class='topic-container'>\"\n",
    "        html_content += f\"<h2>Topic {topic_num}</h2>\"\n",
    "        html_content += \"<p class='topic-words'><strong>Top Words: </strong>\" + \", \".join([word for word, _ in topic_words]) + \"</p>\"\n",
    "        html_content += \"<div class='video-container'>\"\n",
    "        \n",
    "        for _, row in topic_videos.iterrows():\n",
    "            video_path = row['video_path']\n",
    "            first_frame_filename = os.path.basename(video_path).replace(\".mp4\", \".jpg\")\n",
    "            first_frame_path = os.path.join(output_dir, first_frame_filename)\n",
    "            extract_first_frame(video_path, first_frame_path)\n",
    "            html_content += f\"<div class='video-item'><img src='{first_frame_path}' alt='First frame'></div>\"\n",
    "\n",
    "        html_content += \"</div></div>\"\n",
    "\n",
    "    html_content += \"</body></html>\"\n",
    "    \n",
    "    with open(os.path.join(output_dir, \"topics.html\"), \"w\") as f:\n",
    "        f.write(html_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288e934b-b65a-4341-88fd-7a14448a1521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming videoData and topic_model are already defined\n",
    "output_directory = \"topics_visualization\"\n",
    "create_html_for_topics(videoData, topic_model, output_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
